import os
import shutil
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Milvus
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.chat_models import ChatTongyi
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

class SmartCSBotMilvus:
    def __init__(self, data_dir="./data", model_name="qwen-max"):
        self.data_dir = data_dir
        self.model_name = model_name
        self.vectorstore = None
        self.qa_chain = None
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            input_key="question", 
            output_key="answer"
        )
        
        # å¼ºåˆ¶ç¦ç”¨ä»£ç†
        os.environ.pop('http_proxy', None)
        os.environ.pop('https_proxy', None)
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        self.init_vectorstore()
        
    def init_vectorstore(self):
        """åˆå§‹åŒ–æˆ–åŠ è½½å‘é‡æ•°æ®åº“ (Milvus)"""
        print("ğŸ“¦ æ­£åœ¨è¿æ¥ Milvus å¹¶åŠ è½½çŸ¥è¯†åº“...")
        
        # 1. æ£€æŸ¥æ•°æ®ç›®å½•
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
            print(f"âš ï¸ è­¦å‘Š: æ•°æ®ç›®å½• {self.data_dir} ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºã€‚è¯·æ”¾å…¥ txt æ–‡ä»¶ã€‚")
            # å³ä½¿æ²¡æœ‰æ–‡ä»¶ï¼Œæˆ‘ä»¬ä¹Ÿå°è¯•è¿æ¥ Milvusï¼Œå¯èƒ½ä¹‹å‰å·²ç»å­˜è¿‡æ•°æ®
        
        # 2. åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰ txt æ–‡ä»¶
        loader = DirectoryLoader(self.data_dir, glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
        docs = loader.load()
        
        # 3. é…ç½® Embedding
        embeddings = DashScopeEmbeddings(model="text-embedding-v1")

        if docs:
            print(f"ğŸ“„ å‘ç° {len(docs)} ä¸ªæ–‡æ¡£ï¼Œæ­£åœ¨å¤„ç†...")
            # åˆ‡åˆ†æ–‡æ¡£
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
            texts = text_splitter.split_documents(docs)
            
            # åˆ›å»º/æ›´æ–° Milvus é›†åˆ
            # Milvus ä¼šè‡ªåŠ¨å¤„ç†å¢é‡æ’å…¥ (ä½†åœ¨ç”Ÿäº§ç¯å¢ƒéœ€è¦æ›´å¤æ‚çš„å»é‡é€»è¾‘)
            # è¿™é‡Œç®€å•æ¼”ç¤ºï¼šå¦‚æœæœ‰æ–°æ–‡æ¡£å°±æ’å…¥
            self.vectorstore = Milvus.from_documents(
                texts,
                embeddings,
                collection_name="rag_knowledge_base",
                connection_args={"host": "localhost", "port": "19530"}
            )
            print(f"âœ… å·²å‘ Milvus æ’å…¥ {len(texts)} ä¸ªæ–‡æ¡£å—ã€‚")
        else:
            print("âš ï¸ æœ¬åœ° data ç›®å½•ä¸ºç©ºï¼Œå°è¯•ç›´æ¥è¿æ¥ç°æœ‰çš„ Milvus é›†åˆ...")
            # å¦‚æœæœ¬åœ°æ²¡æ–‡ä»¶ï¼Œå°è¯•ç›´æ¥è¿æ¥
            try:
                self.vectorstore = Milvus(
                    embedding_function=embeddings,
                    collection_name="rag_knowledge_base",
                    connection_args={"host": "localhost", "port": "19530"}
                )
                print("âœ… æˆåŠŸè¿æ¥åˆ°ç°æœ‰ Milvus é›†åˆã€‚")
            except Exception as e:
                print(f"âŒ è¿æ¥ Milvus å¤±è´¥: {e}")
                return

        # åˆ›å»º QA é“¾
        self.init_qa_chain()
        print("âœ… RAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

    def init_qa_chain(self):
        """åˆå§‹åŒ–é—®ç­”é“¾"""
        if not self.vectorstore:
            return
            
        llm = ChatTongyi(model=self.model_name)
        
        prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†æ™ºèƒ½å®¢æœâ€œå°èœœâ€ã€‚
è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·ç¤¼è²Œåœ°å›ç­”â€œæŠ±æ­‰ï¼Œæš‚æ—¶æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è”ç³»äººå·¥å®¢æœâ€ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
å›ç­”è¦äº²åˆ‡ã€è‡ªç„¶ï¼Œå¯ä»¥ä½¿ç”¨ emojiã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

ç”¨æˆ·æé—®ï¼š{question}
å®¢æœå›ç­”ï¼š"""

        PROMPT = PromptTemplate(
            template=prompt_template, 
            input_variables=["context", "question"]
        )
        
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
            memory=self.memory,
            combine_docs_chain_kwargs={"prompt": PROMPT}
        )

    def chat(self, query):
        """å¯¹å¤–æä¾›çš„å¯¹è¯æ¥å£"""
        if not self.qa_chain:
            return "âš ï¸ ç³»ç»Ÿæœªå°±ç»ªï¼šè¯·ç¡®ä¿ Milvus æ­£åœ¨è¿è¡Œä¸”å·²å­˜å…¥æ•°æ®ã€‚"
            
        try:
            result = self.qa_chain.invoke({"question": query})
            return result['answer']
        except Exception as e:
            return f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"

    def clear_memory(self):
        self.memory.clear()

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸš€ å¯åŠ¨ Milvus RAG æµ‹è¯•...")
    bot = SmartCSBotMilvus(data_dir="./data")
    
    # æ¨¡æ‹Ÿå¯¹è¯
    questions = ["ä½ ä»¬æ”¯æŒé€€è´§å—ï¼Ÿ", "å¤šå°‘é’±åŒ…é‚®ï¼Ÿ"]
    for q in questions:
        print(f"\nUser: {q}")
        ans = bot.chat(q)
        print(f"Bot: {ans}")
