<div style="display: flex; align-items: flex-start;">

<!-- 左侧目录 -->
<div style="width: 200px; position: sticky; top: 0; height: 100vh; overflow-y: auto; background-color: #f6f8fa; padding: 20px; border-right: 1px solid #d0d7de; flex-shrink: 0;">

<h3 style="margin-top: 0;">📚 目录导航</h3>

1. [核心成就](#1-核心成就)
2. [技术术语表](#2-技术术语表)
3. [文件清单与作用](#3-文件清单与作用)
4. [流程可视化](#4-流程可视化)
5. [详细操作步骤](#5-详细操作步骤)
   - [环境准备](#51-环境准备)
   - [下载源码](#52-下载源码)
   - [模型转换与量化](#53-模型转换与量化)
   - [极速推理](#54-极速推理)
6. [遇到的坑与解决](#6-遇到的坑与解决)
7. [结果分析](#7-结果分析)

</div>

<!-- 右侧正文 -->
<div style="flex-grow: 1; padding: 20px; min-width: 0;">

# 第20-21周学习笔记：llama.cpp 推理优化

## 1. 核心成就

本周我们达成了一个重要的里程碑：**脱离 Python 和显卡，在纯 CPU 环境下极速运行大模型。**
我们使用 C++ 编写的 `llama.cpp` 工具，将原本庞大的 Qwen 模型压缩并转换格式，实现了在普通笔记本上 **80 token/s** 的飞快生成速度。

## 2. 技术术语表

| 英文 Term | 中文 | 极简解释 |
| :--- | :--- | :--- |
| **GGUF** | GGUF格式 | **通用模型格式**。llama.cpp 专用，把模型的所有文件（权重、词表、参数）打包成一个单文件，加载极快。 |
| **Quantization** | 量化 | **压缩技术**。把模型参数的精度从“高清”（FP16）降到“标清”（Q4/INT4）。体积减小 75%，速度提升 3 倍，且几乎不影响智商。 |
| **Inference** | 推理 | **考试**。模型训练是“学习”，推理是“应用”。即输入问题，模型输出答案的过程。 |
| **Tokens/s** | 每秒Token数 | **手速**。衡量模型生成速度的指标。人类阅读速度约 5-10 t/s，本次我们达到了 81.3 t/s，快到飞起。 |
| **SIMD (AVX2)** | 单指令多数据 | **CPU加速**。CPU 的一种超能力，能一次性处理多个数据。llama.cpp 充分利用了这个能力。 |

## 3. 文件清单与作用

所有文件均位于 `第20_21周` 文件夹下。

| 文件名 | 类型 | 作用 |
| :--- | :--- | :--- |
| `llama-bin/` | 文件夹 | **工具箱**。包含下载好的 `llama-cli.exe` (推理主程序) 和 `llama-quantize.exe` (量化工具)。 |
| `llama.cpp/` | 文件夹 | **源码库**。包含用于转换格式的 Python 脚本 `convert_hf_to_gguf.py`。 |
| `qwen-model/` | 文件夹 | **原始模型**。从 Hugging Face / ModelScope 下载的原始 Qwen 模型文件。 |
| `qwen-0.5b-fp16.gguf` | 文件 | **中间产物**。转换格式后但未压缩的模型，体积较大（约 1GB）。 |
| `qwen-0.5b-q4_k_m.gguf` | 文件 | **最终产物**。经过 4-bit 量化后的模型，体积小（约 300MB），速度快。 |
| `download_model.py` | 脚本 | 自动下载 Qwen 模型的辅助脚本。 |
| `download_llama_source.py` | 脚本 | 自动下载 llama.cpp 源码的辅助脚本。 |

## 4. 流程可视化

```mermaid
graph LR
    A[Hugging Face<br>原始模型] -->|convert_hf_to_gguf.py| B(FP16 GGUF<br>未量化模型)
    B -->|llama-quantize.exe| C(Q4_K_M GGUF<br>量化模型)
    C -->|llama-cli.exe| D[🚀 极速对话]
    
    style B fill:#f9f,stroke:#333
    style C fill:#bfb,stroke:#333
```

## 5. 详细操作步骤

### 5.1 环境准备
*   安装 Python 依赖：`pip install numpy sentencepiece transformers gguf protobuf`
*   **注意**：`numpy` 最好安装预编译版 (`--only-binary=:all:`) 以避免 Windows 下的编译错误。

### 5.2 下载源码
由于 GitHub 连接不稳，我们编写了 `download_llama_source.py` 脚本，直接下载 ZIP 包并解压，绕过了 `git clone` 的网络问题。

### 5.3 模型转换与量化
1.  **转换 (Convert)**：把原始模型转为 GGUF 格式。
    ```bash
    python llama.cpp/convert_hf_to_gguf.py ./qwen-model/qwen/Qwen1.5-0.5B-Chat --outfile qwen-0.5b-fp16.gguf
    ```
2.  **量化 (Quantize)**：压缩为 Q4_K_M 格式。
    ```bash
    .\llama-bin\llama-quantize.exe qwen-0.5b-fp16.gguf qwen-0.5b-q4_k_m.gguf Q4_K_M
    ```

### 5.4 极速推理
启动对话模式：
```bash
.\llama-bin\llama-cli.exe -m qwen-0.5b-q4_k_m.gguf -p "你好" -n 128 -cnv
```

## 6. 遇到的坑与解决

### 坑1：Git Clone 失败
*   **现象**：`Recv failure: Connection was reset`。
*   **解决**：放弃 `git` 协议，直接用 Python 脚本下载 GitHub 的 ZIP 压缩包。

### 坑2：Numpy 安装失败
*   **现象**：`UnicodeDecodeError`，meson 编译报错。
*   **原因**：Windows 中文路径导致源码编译失败。
*   **解决**：强制安装二进制包：`pip install numpy --only-binary=:all:`。

### 坑3：中文输入乱码
*   **现象**：在终端输入中文变成乱码。
*   **解决**：这是 PowerShell 的编码问题。第二次尝试输入时正常，或者可以在运行前执行 `chcp 65001`。

## 7. 结果分析

| 指标 | 原始 PyTorch 运行 | llama.cpp (Q4) 运行 | 提升幅度 |
| :--- | :--- | :--- | :--- |
| **显存占用** | 需显卡或大量内存 | **< 500MB** | ↓ 90% |
| **启动速度** | 约 5-10 秒 | **< 1 秒** | ↑ 10倍 |
| **生成速度** | 较慢 (CPU模式) | **81.3 tokens/s** | ↑↑ 极速 |
| **依赖环境** | 复杂 (PyTorch, CUDA) | **极简 (单文件)** | 极致精简 |

**总结**：llama.cpp 完美诠释了**极简主义**和**第一性原理**——剥离所有不必要的框架和抽象，直接用最底层的代码榨干硬件性能。

</div>
</div>
