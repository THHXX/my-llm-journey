<div style="display: flex; align-items: flex-start;">

<!-- 左侧目录 -->
<div style="width: 200px; position: sticky; top: 0; height: 100vh; overflow-y: auto; background-color: #f6f8fa; padding: 20px; border-right: 1px solid #d0d7de; flex-shrink: 0;">

<h3 style="margin-top: 0;">📚 目录导航</h3>

1. [核心目标](#1-核心目标)
2. [环境准备](#2-环境准备)
3. [详细操作步骤](#3-详细操作步骤)
   - [下载模型](#31-下载模型)
   - [获取工具](#32-获取工具)
   - [转换格式](#33-转换格式)
   - [量化模型](#34-量化模型)
   - [运行推理](#35-运行推理)
4. [常见问题](#4-常见问题)

</div>

<!-- 右侧正文 -->
<div style="flex-grow: 1; padding: 20px; min-width: 0;">

# 第20-21周：推理优化（llama.cpp）

## 1. 核心目标

**让大模型在普通电脑（甚至没有显卡）上飞快运行。**

*   **工具**：`llama.cpp`。这是目前最流行的 C++ 推理库，专门针对 CPU 优化（特别是 Apple Silicon 和 x86 AVX2）。
*   **格式**：`GGUF`。这是 llama.cpp 专用的模型格式，支持单文件存储，加载速度极快。
*   **技术**：`Quantization`（量化）。将模型参数从 16位浮点数（FP16）压缩到 4位整数（Q4），体积减少 75%，速度提升 3-4 倍，精度损失极小。

## 2. 环境准备

*   **系统**：Windows 11
*   **依赖**：Python 3.8+ (用于格式转换)
*   **网络**：需要开启 **科学上网工具** 以访问 GitHub。

## 3. 详细操作步骤

### 3.1 下载模型
为了方便操作，我们先把 Qwen 模型下载到当前目录。请在 `第20_21周` 目录下创建并运行以下脚本。

1. 创建 `download_model.py`：
   ```python
   import os
   from modelscope import snapshot_download

   # 下载 Qwen1.5-0.5B-Chat 模型
   model_dir = snapshot_download('qwen/Qwen1.5-0.5B-Chat', cache_dir='./qwen-model')
   print(f"✅ 模型已下载到: {model_dir}")
   ```
2. 运行脚本：
   ```powershell
   python download_model.py
   ```
   *记下输出的路径，后面会用到。假设路径为 `F:\...\qwen-model\qwen\Qwen1.5-0.5B-Chat`*

### 3.2 获取工具 (llama.cpp)

在 Windows 上编译源码非常麻烦（需要安装 Visual Studio 等）。**极简方案**是直接使用官方编译好的版本，并配合源码中的 Python 脚本进行转换。

**Step A: 下载可执行文件 (运行用)**
1. 访问 [llama.cpp Releases](https://github.com/ggerganov/llama.cpp/releases/latest)。
2. 找到 `llama-bxxxx-bin-win-avx2-x64.zip` (xxxx是版本号，选最新的)。
   * *注：如果你的 CPU 比较老，选 `win-x64`；如果非常新，可以选 `avx512`。通常 `avx2` 最通用。*
3. 下载并解压到 `第20_21周\llama-bin` 文件夹。
   * 确保里面有 `llama-cli.exe` (旧版叫 `main.exe`) 和 `llama-quantize.exe`。

**Step B: 克隆源码 (转换用)**
我们需要源码里的 `convert_hf_to_gguf.py` 脚本。
```powershell
# 在 第20_21周 目录下执行
git clone https://github.com/ggerganov/llama.cpp.git
```

**Step C: 安装转换依赖**
```powershell
cd llama.cpp
pip install -r requirements.txt
cd ..
```

### 3.3 转换格式 (HF -> GGUF)

将 Hugging Face 格式（.safetensors）转换为 GGUF 格式（FP16，未量化）。

*请根据 3.1 步实际下载的路径修改下面的 `--src_dir`*

```powershell
# 语法: python llama.cpp/convert_hf_to_gguf.py [模型路径] --outfile [输出文件名]

python llama.cpp/convert_hf_to_gguf.py ./qwen-model/qwen/Qwen1.5-0.5B-Chat --outfile qwen-0.5b-fp16.gguf
```
*如果成功，当前目录下会生成一个约 1GB 的 `qwen-0.5b-fp16.gguf` 文件。*

### 3.4 量化模型 (FP16 -> Q4_K_M)

使用 `llama-quantize.exe` 将 FP16 模型压缩为 4-bit 量化版。

```powershell
# 语法: .\llama-bin\llama-quantize.exe [输入文件] [输出文件] [量化方法]

.\llama-bin\llama-quantize.exe qwen-0.5b-fp16.gguf qwen-0.5b-q4_k_m.gguf Q4_K_M
```
*   **Q4_K_M**：推荐的量化方法，平衡了速度、大小和精度。
*   生成的 `qwen-0.5b-q4_k_m.gguf` 文件大小会减小到约 300MB-400MB。

### 3.5 运行推理

终于到了见证奇迹的时刻！使用 `llama-cli.exe` 运行模型。

```powershell
# 语法: .\llama-bin\llama-cli.exe -m [模型文件] -p [提示词] -n [生成长度]

.\llama-bin\llama-cli.exe -m qwen-0.5b-q4_k_m.gguf -p "你好，请介绍一下你自己" -n 128 -cnv
```

**参数说明：**
*   `-m`: 指定模型文件。
*   `-p`: Prompt，提示词。
*   `-n`: 预测生成的 Token 数量。
*   `-cnv`: (Conversation) 开启对话模式，可以像聊天一样互动（Ctrl+C 退出）。
*   `-c`: 上下文窗口大小，默认 512，可以设大点比如 2048。

## 4. 常见问题

1.  **报错 `ImportError: No module named 'gguf'`**
    *   原因：没安装依赖。
    *   解决：确保执行了 `pip install -r llama.cpp/requirements.txt`。

2.  **乱码问题**
    *   原因：Windows 终端编码问题。
    *   解决：在运行命令前执行 `chcp 65001` 切换到 UTF-8 编码。

3.  **下载速度慢**
    *   请确保开启了科学上网工具，或者使用 IDM 等工具下载 Release 包。

</div>
</div>
