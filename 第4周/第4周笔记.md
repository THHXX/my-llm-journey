## 第4周：文本向量化与相似度计算笔记

### 一、本周目标

- **理解什么是 Embedding（向量表示）**，知道它和「会聊天的 LLM」的区别。
- **学会用 sentence-transformers 生成中文句向量**。
- **使用余弦相似度衡量两句话的语义相似度，并理解相似度数值的含义。**

### 二、核心概念

- **Embedding（文本向量表示）**
  - 把一句话/一段文本转换成一个高维向量（例如 384 维）。
  - 向量之间的角度/距离可以表示语义相似度，便于计算「像不像」。

- **句向量模型 vs 聊天大模型**
  - 句向量模型（如 `all-MiniLM-L6-v2`）：  
    - 输入：一句话/一段话  
    - 输出：一个固定长度的向量  
    - 主要用途：相似度计算、检索、聚类。
  - 聊天大模型（如 Qwen、DeepSeek）：  
    - 输入：上下文对话  
    - 输出：新的文本（回答、代码等）  
    - 主要用途：生成内容。

- **all-MiniLM-L6-v2 特点（大致）**
  - Transformer 结构的小模型，参数量在**几千万级**，远小于主流 LLM。
  - 输出 384 维向量，体积在几十 MB 量级，适合本地快速实验。
  - 支持中英文句子，相似度结果对中文也有一定效果。

- **余弦相似度（cosine similarity）**
  - 衡量两个向量的夹角是否接近：
    - 接近 1：方向相似，语义相近。
    - 接近 0：几乎无关。
    - 接近 -1：方向相反（在文本场景里很少出现）。
  - 常用经验值：  
    - > 0.7：通常可以认为语义比较接近。  

### 三、示例代码结构（`embedding_demo.py`）

- **加载模型与编码**

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

def main() -> None:
    model_name = "sentence-transformers/all-MiniLM-L6-v2"  # 句向量模型名称
    model = SentenceTransformer(model_name)  # 加载模型（首次需联网下载）

    sent1 = "我喜欢学习大模型"
    sent2 = "我对 LLM 很感兴趣"

    emb1 = model.encode(sent1)  # 把句子1编码成向量
    emb2 = model.encode(sent2)  # 把句子2编码成向量

    sim = cosine_similarity([emb1], [emb2])[0][0]  # 计算余弦相似度
    print(f"余弦相似度: {sim:.4f}")
```

- **运行方式**
  - 确认虚拟环境已激活后：
    - `pip install sentence-transformers scikit-learn  # 安装依赖`
    - `python 第4周\embedding_demo.py  # 计算两句中文的相似度`

### 四、实验现象与理解

- **第一次运行模型下载问题**
  - 一开始 Hugging Face 模型仓库 `huggingface.co` 连接超时，模型文件无法下载；
  - 后来网络恢复后，看到诸如 `model.safetensors 90.9M` 的进度条，说明模型已成功缓存到：
    - `C:\Users\你的用户名\.cache\huggingface\...`
  - 下载完成后，后续运行不再重复下载，只是从本地加载。

- **相似度结果示例**
  - 句子：
    - `sent1 = "我喜欢学习大模型"`
    - `sent2 = "我对 LLM 很感兴趣"`
  - 实际跑出的相似度约为 **0.65 左右**（例如 0.6529）。
  - 虽然没达到 0.7，但已经说明：模型认为两句话语义接近。
  - 经验结论：**0.7 只是经验阈值，不是刚性标准，要结合任务和模型理解。**

- **对比更多例子可以帮助理解**
  - 更相似一组（相似度应更高）：
    - `sent1 = "我喜欢学习大模型"`
    - `sent2 = "我很喜欢研究大语言模型"`
  - 明显不相似一组（相似度应更低）：
    - `sent1 = "我喜欢学习大模型"`
    - `sent2 = "今天天气很好，我想去爬山"`
  - 实践中可以多试几组，对比相似度数值和直觉的关系。

### 五、本周踩坑与收获

- **踩坑**
  - Hugging Face 网络访问超时，导致模型下载失败，卡在重试；  
  - 通过阅读报错信息，理解了：  
    - sentence-transformers 首次需要从 `huggingface.co` 下载模型；
    - 模型缓存目录在用户家目录的 `.cache` 下。

- **收获**
  - 理解了：**Embedding = 把文本变成「可比较」的数字向量**，是后面构建向量检索（RAG）的基础。
  - 实战中亲手跑通了从「句子 → 向量 → 相似度」的完整流程。
  - 对相似度数值有了直觉：  
    - 0.6–0.7：通常表示语义接近；  
    - 换不同任务/模型时，需要重新校准阈值。

### 六、为后续 RAG 做的准备

- 第 4 周的重点成果，为第 5–6 周的 RAG 系统打下基础：
  - 已经会用 sentence-transformers 生成句向量；
  - 知道如何用余弦相似度衡量文本相似度；
  - 理解了「模型下载与缓存」的流程和可能遇到的网络问题。


