import os
import chromadb
from chromadb.utils import embedding_functions
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pdf_parser import extract_text_from_pdf
from llm_interface import LocalLLM

# é…ç½®è·¯å¾„
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(os.path.dirname(CURRENT_DIR), "data")
CHROMA_DB_DIR = os.path.join(os.path.dirname(CURRENT_DIR), "chroma_db")

def build_vector_db(pdf_filename="tesla_2023_10k.pdf", collection_name="financial_reports"):
    """
    æ„å»ºå‘é‡æ•°æ®åº“ï¼šè¯»å–PDF -> åˆ‡åˆ† -> åµŒå…¥ -> å­˜å…¥ ChromaDB
    """
    pdf_path = os.path.join(DATA_DIR, pdf_filename)
    if not os.path.exists(pdf_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {pdf_path}")
        return None

    # 1. æå–æ–‡æœ¬
    print(f"1. æ­£åœ¨è¯»å– {pdf_filename} ...")
    full_text = extract_text_from_pdf(pdf_path)
    if not full_text:
        print("æå–æ–‡æœ¬å¤±è´¥ã€‚")
        return None
    print(f"   æå–æˆåŠŸï¼Œå…± {len(full_text)} å­—ç¬¦ã€‚")

    # 2. æ–‡æœ¬åˆ‡åˆ† (Chunking)
    # ä½¿ç”¨ RecursiveCharacterTextSplitter æ™ºèƒ½åˆ‡åˆ†
    print("2. æ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ‡åˆ†...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,      # æ¯ä¸ªå—çº¦ 1000 å­—ç¬¦
        chunk_overlap=200,    # é‡å  200 å­—ç¬¦ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡ä¸¢å¤±
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = text_splitter.split_text(full_text)
    print(f"   åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

    # åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯
    print(f"3. åˆå§‹åŒ– ChromaDB (æŒä¹…åŒ–è·¯å¾„: {CHROMA_DB_DIR})...")
    client = chromadb.PersistentClient(path=CHROMA_DB_DIR)
    
    # -----------------------------------------------------------
    # ä½¿ç”¨ ModelScope (é˜¿é‡Œäº‘) ä¸‹è½½æ¨¡å‹ï¼Œè§£å†³å›½å†…ç½‘ç»œé—®é¢˜
    # -----------------------------------------------------------
    # ã€é‡è¦ã€‘è®¾ç½®ç¼“å­˜ç›®å½•åˆ° D ç›˜ï¼Œé¿å…å ç”¨ C ç›˜ç©ºé—´
    os.environ['MODELSCOPE_CACHE'] = 'D:\\ModelScope_Cache'
    
    model_name_or_path = "all-MiniLM-L6-v2" # é»˜è®¤å€¼
    try:
        from modelscope import snapshot_download
        print("   ğŸš€ æ­£åœ¨ä½¿ç”¨ ModelScope (é˜¿é‡Œäº‘) ä¸‹è½½æ¨¡å‹...")
        print(f"   ğŸ“‚ ç¼“å­˜ç›®å½•: {os.environ['MODELSCOPE_CACHE']}")
        # è¿™é‡Œçš„ model_id æ˜¯ ModelScope ä¸Šçš„é•œåƒ ID
        model_dir = snapshot_download('AI-ModelScope/all-MiniLM-L6-v2', revision='master')
        model_name_or_path = model_dir
        print(f"   âœ… æ¨¡å‹å·²ä¸‹è½½è‡³: {model_dir}")
    except Exception as e:
        print(f"   âš ï¸ ModelScope ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ç›´æ¥åŠ è½½ ({e})")

    # ä½¿ç”¨ sentence-transformers åŠ è½½æœ¬åœ°æ¨¡å‹
    emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=model_name_or_path
    )

    # è·å–æˆ–åˆ›å»ºé›†åˆ (Collection)
    # å¦‚æœå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤é‡å»º (ä¸ºäº†æµ‹è¯•æ–¹ä¾¿ï¼Œå®é™…ç”Ÿäº§å¯ä»¥å¢é‡æ›´æ–°)
    try:
        # å…ˆæ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆ é™¤
        try:
            client.get_collection(name=collection_name)
            client.delete_collection(name=collection_name)
            print(f"   å·²åˆ é™¤æ—§é›†åˆ {collection_name}")
        except:
            pass # é›†åˆä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤
    except Exception as e:
        print(f"   æ¸…ç†é›†åˆæ—¶å‡ºé”™: {e}")

    collection = client.create_collection(
        name=collection_name,
        embedding_function=emb_fn
    )

    # 4. æ‰¹é‡å­˜å…¥æ•°æ®
    print("4. æ­£åœ¨ç”Ÿæˆå‘é‡å¹¶å­˜å…¥æ•°æ®åº“ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    
    # æ„é€  ID å’Œå…ƒæ•°æ®
    ids = [f"chunk_{i}" for i in range(len(chunks))]
    metadatas = [{"source": pdf_filename, "chunk_index": i} for i in range(len(chunks))]
    
    # ChromaDB å»ºè®®åˆ†æ‰¹æ’å…¥ï¼Œé˜²æ­¢ä¸€æ¬¡æ€§è¿‡å¤§
    batch_size = 100
    total_batches = (len(chunks) + batch_size - 1) // batch_size
    
    for i in range(0, len(chunks), batch_size):
        batch_end = min(i + batch_size, len(chunks))
        print(f"   æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{total_batches} (Chunk {i} - {batch_end})...")
        collection.add(
            documents=chunks[i:batch_end],
            metadatas=metadatas[i:batch_end],
            ids=ids[i:batch_end]
        )
        
    print("âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
    return collection

def query_vector_db(query_text, collection_name="financial_reports", n_results=3):
    """
    æŸ¥è¯¢å‘é‡æ•°æ®åº“
    """
    print(f"\nğŸ” æ­£åœ¨æŸ¥è¯¢: '{query_text}' ...")
    
    client = chromadb.PersistentClient(path=CHROMA_DB_DIR)
    
    # -----------------------------------------------------------
    # åŒæ ·ä½¿ç”¨ ModelScope è·¯å¾„åŠ è½½æ¨¡å‹
    # -----------------------------------------------------------
    os.environ['MODELSCOPE_CACHE'] = 'D:\\ModelScope_Cache'
    model_name_or_path = "all-MiniLM-L6-v2"
    try:
        from modelscope import snapshot_download
        # æ­¤æ—¶åº”è¯¥å·²ç»ç¼“å­˜äº†ï¼Œä¸ä¼šé‡å¤ä¸‹è½½
        model_dir = snapshot_download('AI-ModelScope/all-MiniLM-L6-v2', revision='master')
        model_name_or_path = model_dir
    except:
        pass

    emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=model_name_or_path
    )
    
    try:
        collection = client.get_collection(name=collection_name, embedding_function=emb_fn)
        results = collection.query(
            query_texts=[query_text],
            n_results=n_results
        )
        
        print(f"   æ‰¾åˆ° {len(results['documents'][0])} ä¸ªç›¸å…³ç‰‡æ®µ:\n")
        for i, doc in enumerate(results['documents'][0]):
            meta = results['metadatas'][0][i]
            print(f"   [ç‰‡æ®µ {i+1}] (æ¥æº: {meta['source']}, Index: {meta['chunk_index']})")
            print(f"   {'-'*50}")
            print(f"   {doc[:200]}...") # åªæ˜¾ç¤ºå‰200å­—ç¬¦
            print(f"   {'-'*50}\n")
            
    except Exception as e:
        print(f"æŸ¥è¯¢å‡ºé”™: {e}")
        return []
    
    return results['documents'][0]

def rag_chat(query_text, collection_name="financial_reports"):
    """
    RAG å¯¹è¯ï¼šæ£€ç´¢ -> ç”Ÿæˆ
    """
    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    retrieved_docs = query_vector_db(query_text, collection_name, n_results=3)
    
    if not retrieved_docs:
        print("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•å›ç­”ã€‚")
        return

    # 2. æ„å»º Prompt (é’ˆå¯¹å°æ¨¡å‹ä¼˜åŒ–ï¼šç®€æ´æŒ‡ä»¤)
    context = "\n\n".join(retrieved_docs)
    
    system_prompt = """You are a Financial Analyst. Analyze the Context to answer the Question.
    
Rules:
1. Only use the provided Context.
2. If the answer is not in Context, say "Data not available".
3. Keep answers concise and professional.
4. Use bullet points for lists.
"""
    
    user_prompt = f"""
Context:
{context}

Question: 
{query_text}

Answer:
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # 3. è°ƒç”¨ LLM
    print(f"\nğŸ¤– æ­£åœ¨è¯·æ±‚ LLM ç”Ÿæˆå›ç­”...")
    try:
        llm = LocalLLM() # ä¼šè‡ªåŠ¨ç¡®ä¿æœåŠ¡è¿è¡Œ
        response = llm.chat(messages, temperature=0.1) # ä½æ¸©åº¦ä»¥ä¿è¯åŸºäºäº‹å®
        
        if response:
            answer = response['choices'][0]['message']['content']
            print(f"\n{'='*20} ğŸ¤– AI å›ç­” {'='*20}")
            print(answer)
            print(f"{'='*50}\n")
            return answer
        else:
            msg = "LLM æœªè¿”å›æœ‰æ•ˆç»“æœã€‚"
            print(msg)
            return msg
            
    except Exception as e:
        msg = f"è°ƒç”¨ LLM å¤±è´¥: {e}"
        print(msg)
        return msg

if __name__ == "__main__":
    # 1. æ„å»ºåº“ (å¦‚æœç¬¬ä¸€æ¬¡è¿è¡Œ)
    # æ³¨æ„ï¼šå¦‚æœåªæƒ³æµ‹è¯•æŸ¥è¯¢ï¼Œå¯ä»¥æ³¨é‡Šæ‰ build_vector_db
    # build_vector_db()
    
    # 2. RAG æµ‹è¯•
    print("\n" + "="*50)
    question = "What is Tesla's total revenue in 2023?"
    rag_chat(question)
    
    print("\n" + "="*50)
    question2 = "What are the risk factors mentioned?"
    rag_chat(question2)
