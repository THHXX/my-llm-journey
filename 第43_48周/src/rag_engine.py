import os
import chromadb
from chromadb.utils import embedding_functions
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pdf_parser import extract_text_from_pdf, extract_text_with_page_infos
from llm_interface import LocalLLM, QwenCloudLLM
import datetime

# é…ç½®è·¯å¾„
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(os.path.dirname(CURRENT_DIR), "data")
CHROMA_DB_DIR = os.path.join(os.path.dirname(CURRENT_DIR), "chroma_db")

def build_vector_db(pdf_filename="tesla_2023_10k.pdf", collection_name="financial_reports"):
    """
    æ„å»ºå‘é‡æ•°æ®åº“ï¼šè¯»å–PDF -> åˆ‡åˆ† -> åµŒå…¥ -> å­˜å…¥ ChromaDB
    """
    pdf_path = os.path.join(DATA_DIR, pdf_filename)
    if not os.path.exists(pdf_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {pdf_path}")
        return None

    # 1. æå–æ–‡æœ¬ (å¸¦é¡µç )
    print(f"1. æ­£åœ¨è¯»å– {pdf_filename} ...")
    # full_text = extract_text_from_pdf(pdf_path) # æ—§æ¥å£
    pages_data = extract_text_with_page_infos(pdf_path) # æ–°æ¥å£: [{"page": 1, "text": "..."}, ...]
    
    if not pages_data:
        print("æå–æ–‡æœ¬å¤±è´¥ã€‚")
        return None
    print(f"   æå–æˆåŠŸï¼Œå…± {len(pages_data)} é¡µã€‚")

    # 2. æ–‡æœ¬åˆ‡åˆ† (Chunking) - æŒ‰é¡µåˆ‡åˆ†ä»¥ä¿ç•™é¡µç 
    print("2. æ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ‡åˆ† (Page-wise Chunking)...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,      # ç¨å¾®å‡å°ä¸€ç‚¹ï¼Œå› ä¸ºæ˜¯å•é¡µåˆ‡åˆ†
        chunk_overlap=200,
        separators=["\n\n", "\n", " ", ""]
    )
    
    all_chunks = []
    all_metadatas = []
    all_ids = []
    
    chunk_counter = 0
    for page_info in pages_data:
        page_num = page_info['page']
        page_text = page_info['text']
        
        page_chunks = text_splitter.split_text(page_text)
        
        for chunk in page_chunks:
            all_chunks.append(chunk)
            all_metadatas.append({
                "source": pdf_filename,
                "page": page_num, # å…³é”®ï¼šå­˜å…¥é¡µç ï¼
                "chunk_index": chunk_counter
            })
            all_ids.append(f"chunk_{chunk_counter}")
            chunk_counter += 1
            
    print(f"   åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

    # åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯
    print(f"3. åˆå§‹åŒ– ChromaDB (æŒä¹…åŒ–è·¯å¾„: {CHROMA_DB_DIR})...")
    client = chromadb.PersistentClient(path=CHROMA_DB_DIR)
    
    # -----------------------------------------------------------
    # ä½¿ç”¨ ModelScope (é˜¿é‡Œäº‘) ä¸‹è½½æ¨¡å‹ï¼Œè§£å†³å›½å†…ç½‘ç»œé—®é¢˜
    # -----------------------------------------------------------
    # ã€é‡è¦ã€‘è®¾ç½®ç¼“å­˜ç›®å½•åˆ° D ç›˜ï¼Œé¿å…å ç”¨ C ç›˜ç©ºé—´
    os.environ['MODELSCOPE_CACHE'] = 'D:\\ModelScope_Cache'
    
    # ğŸŒŸ å‡çº§ä¸º BGE-M3 (ä¸­æ–‡/å¤šè¯­è¨€æ£€ç´¢æœ€å¼º)
    # è™½ç„¶æ¨¡å‹ç¨å¤§ (çº¦ 2GB)ï¼Œä½†æ•ˆæœè´¨å˜ï¼Œæ”¯æŒä¸­è‹±æ··åˆæ£€ç´¢
    model_id = "Xorbits/bge-m3" 
    model_name_or_path = "BAAI/bge-m3" # fallback name

    try:
        from modelscope import snapshot_download
        print("   ğŸš€ æ­£åœ¨ä½¿ç”¨ ModelScope (é˜¿é‡Œäº‘) ä¸‹è½½ BGE-M3 æ¨¡å‹...")
        print(f"   ğŸ“‚ ç¼“å­˜ç›®å½•: {os.environ['MODELSCOPE_CACHE']}")
        model_dir = snapshot_download(model_id, revision='master')
        model_name_or_path = model_dir
        print(f"   âœ… æ¨¡å‹å·²ä¸‹è½½è‡³: {model_dir}")
    except Exception as e:
        print(f"   âš ï¸ ModelScope ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ç›´æ¥åŠ è½½ ({e})")

    # ä½¿ç”¨ sentence-transformers åŠ è½½æœ¬åœ°æ¨¡å‹
    emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=model_name_or_path
    )

    # è·å–æˆ–åˆ›å»ºé›†åˆ (Collection)
    # å¦‚æœå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤é‡å»º (ä¸ºäº†æµ‹è¯•æ–¹ä¾¿ï¼Œå®é™…ç”Ÿäº§å¯ä»¥å¢é‡æ›´æ–°)
    try:
        # å…ˆæ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆ é™¤
        try:
            client.get_collection(name=collection_name)
            client.delete_collection(name=collection_name)
            print(f"   å·²åˆ é™¤æ—§é›†åˆ {collection_name}")
        except:
            pass # é›†åˆä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤
    except Exception as e:
        print(f"   æ¸…ç†é›†åˆæ—¶å‡ºé”™: {e}")

    collection = client.create_collection(
        name=collection_name,
        embedding_function=emb_fn
    )

    # 4. æ‰¹é‡å­˜å…¥æ•°æ®
    print("4. æ­£åœ¨ç”Ÿæˆå‘é‡å¹¶å­˜å…¥æ•°æ®åº“ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    
    # ChromaDB å»ºè®®åˆ†æ‰¹æ’å…¥ï¼Œé˜²æ­¢ä¸€æ¬¡æ€§è¿‡å¤§
    batch_size = 100
    total_batches = (len(all_chunks) + batch_size - 1) // batch_size
    
    for i in range(0, len(all_chunks), batch_size):
        batch_end = min(i + batch_size, len(all_chunks))
        print(f"   æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{total_batches}...")
        collection.add(
            documents=all_chunks[i:batch_end],
            metadatas=all_metadatas[i:batch_end],
            ids=all_ids[i:batch_end]
        )
        
    print("âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
    return collection

def query_vector_db(query_text, collection_name="financial_reports", n_results=3):
    """
    æŸ¥è¯¢å‘é‡æ•°æ®åº“
    """
    print(f"\nğŸ” æ­£åœ¨æŸ¥è¯¢: '{query_text}' ...")
    
    client = chromadb.PersistentClient(path=CHROMA_DB_DIR)
    
    # -----------------------------------------------------------
    # åŒæ ·ä½¿ç”¨ ModelScope è·¯å¾„åŠ è½½æ¨¡å‹
    # -----------------------------------------------------------
    os.environ['MODELSCOPE_CACHE'] = 'D:\\ModelScope_Cache'
    model_id = "Xorbits/bge-m3" 
    model_name_or_path = "BAAI/bge-m3"

    try:
        from modelscope import snapshot_download
        # æ­¤æ—¶åº”è¯¥å·²ç»ç¼“å­˜äº†ï¼Œä¸ä¼šé‡å¤ä¸‹è½½
        model_dir = snapshot_download(model_id, revision='master')
        model_name_or_path = model_dir
    except:
        pass

    emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=model_name_or_path
    )
    
    try:
        collection = client.get_collection(name=collection_name, embedding_function=emb_fn)
        results = collection.query(
            query_texts=[query_text],
            n_results=n_results
        )
        
        print(f"   æ‰¾åˆ° {len(results['documents'][0])} ä¸ªç›¸å…³ç‰‡æ®µ:\n")
        for i, doc in enumerate(results['documents'][0]):
            meta = results['metadatas'][0][i]
            print(f"   [ç‰‡æ®µ {i+1}] (æ¥æº: {meta['source']}, Index: {meta['chunk_index']})")
            print(f"   {'-'*50}")
            print(f"   {doc[:200]}...") # åªæ˜¾ç¤ºå‰200å­—ç¬¦
            print(f"   {'-'*50}\n")
            
    except Exception as e:
        print(f"æŸ¥è¯¢å‡ºé”™: {e}")
        return [], []
    
    return results['documents'][0], results['metadatas'][0]

def rag_chat(query_text, collection_name="financial_reports", llm_type="local"):
    """
    RAG å¯¹è¯ï¼šæ£€ç´¢ -> ç”Ÿæˆ
    :param llm_type: 'local' (æœ¬åœ°LLM) æˆ– 'cloud' (é˜¿é‡Œäº‘Qwen)
    """
    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    # ğŸ’¡ ç­–ç•¥ä¼˜åŒ–ï¼šCloud æ¨¡å‹ (Qwen-Max) ä¸Šä¸‹æ–‡çª—å£å¾ˆå¤§ï¼Œå¯ä»¥æ£€ç´¢æ›´å¤šæ–‡æ¡£ä»¥æé«˜å¬å›ç‡
    # Local æ¨¡å‹ä¸Šä¸‹æ–‡æœ‰é™ï¼Œä¿æŒè¾ƒå°‘çš„æ£€ç´¢æ•°é‡
    target_n_results = 10 if llm_type == "cloud" else 3
    
    retrieved_docs, metadatas = query_vector_db(query_text, collection_name, n_results=target_n_results)
    
    if not retrieved_docs:
        print("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•å›ç­”ã€‚")
        return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•å›ç­”ã€‚" # Return string for UI

    # 2. æ„å»º Prompt (é’ˆå¯¹å°æ¨¡å‹ä¼˜åŒ–ï¼šä¸­æ–‡æŒ‡ä»¤ï¼Œå¼ºåˆ¶ç®€çŸ­)
    context = "\n\n".join(retrieved_docs)
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆã€‚è¯·æ ¹æ®æä¾›çš„ã€ä¸Šä¸‹æ–‡ã€‘å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚

è§„åˆ™ï¼š
1. ä¸Šä¸‹æ–‡å¯èƒ½æ˜¯è‹±æ–‡çš„ï¼Œè¯·ä½ ç†è§£åç”¨**ä¸­æ–‡**å›ç­”ã€‚
2. å¿…é¡»å®Œå…¨åŸºäºã€ä¸Šä¸‹æ–‡ã€‘ä¸­çš„ä¿¡æ¯å›ç­”ï¼Œä¸è¦ä½¿ç”¨ä½ è‡ªå·±çš„å¤–éƒ¨çŸ¥è¯†ã€‚
3. å¦‚æœã€ä¸Šä¸‹æ–‡ã€‘ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥å›ç­”â€œæ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ã€‚
4. ä¸¥ç¦ç¼–é€ æ•°å­—æˆ–äº‹å®ã€‚
5. ã€é‡è¦ã€‘æ³¨æ„æ•°å­—å•ä½ï¼å¦‚æœæ–‡ä¸­å•ä½æ˜¯ "in millions" (ç™¾ä¸‡)ï¼Œè¯·åœ¨å›ç­”ä¸­æ˜ç¡®æŒ‡å‡º (ä¾‹å¦‚ï¼š$78,509 million æˆ– 785.09äº¿ç¾å…ƒ)ã€‚
6. ä¿æŒå›ç­”ç®€æ´æ˜äº†ï¼Œç›´æ¥ç»™å‡ºç»“è®ºæˆ–æ•°å­—ã€‚
7. å›ç­”ç»“æŸåï¼Œè¯·åŠ¡å¿…è¾“å‡º "[END]" å¹¶åœæ­¢ã€‚

ã€è¿›é˜¶åŠŸèƒ½ - å›¾è¡¨ç”Ÿæˆåè®®ã€‘ï¼šï¼ˆèƒ½å¤Ÿç”Ÿæˆå›¾è¡¨å°±ä¸€å®šè¦æœ‰jsonï¼‰
å¦‚æœç”¨æˆ·çš„é—®é¢˜æ¶‰åŠ**æ•°æ®å¯¹æ¯”**æˆ–**è¶‹åŠ¿åˆ†æ**ï¼Œä¸”ä¸Šä¸‹æ–‡ä¸­åŒ…å«è¶³å¤Ÿçš„æ•°æ®ï¼Œè¯·åœ¨å›ç­”çš„æœ€åï¼ˆ[END]ä¹‹å‰ï¼‰é™„å¸¦ä¸€ä¸ª JSON ä»£ç å—ï¼Œç”¨äºç”Ÿæˆå›¾è¡¨ã€‚
æ ¼å¼å¦‚ä¸‹ï¼š
```json
{
    "type": "bar",  // å›¾è¡¨ç±»å‹: "bar" (æŸ±çŠ¶å›¾), "line" (æŠ˜çº¿å›¾)
    "title": "å›¾è¡¨æ ‡é¢˜",
    "data": {
        "x": ["2021", "2022", "2023"], // Xè½´æ ‡ç­¾
        "y": [100, 200, 300],          // Yè½´æ•°å€¼ (çº¯æ•°å­—ï¼Œä¸è¦å¸¦å•ä½)
        "x_label": "å¹´ä»½",             // Xè½´åç§°
        "y_label": "æ”¶å…¥ (ç™¾ä¸‡ç¾å…ƒ)"    // Yè½´åç§°
    }
}
```
æ³¨æ„ï¼š
- ä»…åœ¨æ•°æ®å……è¶³æ—¶ç”Ÿæˆã€‚
- JSON å¿…é¡»åŒ…è£¹åœ¨ ```json å’Œ ``` ä¹‹é—´ã€‚
- Yè½´æ•°æ®å¿…é¡»æ˜¯çº¯æ•°å­—ã€‚
"""
    
    user_prompt = f"""
ã€ä¸Šä¸‹æ–‡ã€‘ï¼š
{context}

ã€é—®é¢˜ã€‘ï¼š
{query_text}

ã€å›ç­”ã€‘ï¼š
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # 3. è°ƒç”¨ LLM
    print(f"\nğŸ¤– æ­£åœ¨è¯·æ±‚ LLM ({llm_type}) ç”Ÿæˆå›ç­”...")
    
    # æ”¶é›†æ¥æºä¿¡æ¯
    sources_text = "\n\n**ğŸ“š å‚è€ƒæ¥æº:**\n"
    seen_pages = set()
    for i, doc in enumerate(retrieved_docs):
        # é‡æ–°æŸ¥è¯¢ metadata (å› ä¸º query_vector_db åªè¿”å›äº† text)
        # è¿™é‡Œä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬å‡è®¾ query_vector_db å†…éƒ¨æ‰“å°äº† metadataï¼Œ
        # ä½†ä¸ºäº†åœ¨ UI å±•ç¤ºï¼Œæœ€å¥½è®© query_vector_db è¿”å›å®Œæ•´å¯¹è±¡ã€‚
        # ç®€åŒ–å¤„ç†ï¼šæˆ‘ä»¬åœ¨ query_vector_db é‡Œç›´æ¥ä¿®æ”¹è¿”å›ç»“æ„ï¼Œæˆ–è€…åœ¨è¿™é‡Œç®€å•é™„åŠ è¯´æ˜ã€‚
        pass
    
    # --- ä¸´æ—¶ä¿®æ­£ï¼šä¸ºäº†è·å– metadataï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹ query_vector_db çš„è¿”å›ç­¾å ---
    # ä½†ä¸ºäº†ä¸ç ´åç°æœ‰é€»è¾‘ï¼Œæˆ‘ä»¬å…ˆæŠŠ sources_text ç•™ç©ºï¼Œ
    # æ›´å¥½çš„åšæ³•æ˜¯ä¿®æ”¹ query_vector_db è¿”å› (docs, metadatas)
    
    try:
        if llm_type == "cloud":
            llm = QwenCloudLLM()
            # äº‘ç«¯æ¨¡å‹èƒ½åŠ›æ›´å¼ºï¼Œtemperature å¯ä»¥ç¨å¾®é«˜ä¸€ç‚¹ç‚¹ï¼Œæˆ–è€…ä¿æŒä½ä½ä»¥æ±‚ç¨³
            response = llm.chat(
                messages,
                temperature=0.2, # ç¨å¾®ç»™ä¸€ç‚¹çµæ´»æ€§ï¼Œä½†ä¿æŒä¸¥è°¨
            )
        else:
            llm = LocalLLM() # ä¼šè‡ªåŠ¨ç¡®ä¿æœåŠ¡è¿è¡Œ
            # é™ä½ temperature åˆ° 0.1 ä»¥å‡å°‘å¹»è§‰
            # å¢åŠ  stop åœæ­¢è¯ï¼Œé˜²æ­¢æ¨¡å‹è‡ªé—®è‡ªç­”
            # å¢åŠ  frequency_penalty é˜²æ­¢é‡å¤å¾ªç¯ (å¦‚ "é²é²é²...")
            response = llm.chat(
                messages, 
                temperature=0.1, 
                stop=["[END]", "ã€é—®é¢˜ã€‘", "User:", "Question:", "\n\n\n"],
                frequency_penalty=1.2 
            ) 
        
        if response:
            answer = response['choices'][0]['message']['content']
            # æ¸…ç†å¯èƒ½çš„ [END] æ ‡è®°
            answer = answer.replace("[END]", "").strip()
            
            print(f"\n{'='*20} ğŸ¤– AI å›ç­” ({llm_type}) {'='*20}")
            print(answer)
            print(f"{'='*50}\n")
            
            # --- æè‡´ä¼˜åŒ–ï¼šè¿½åŠ å¼•ç”¨æ¥æº ---
            # å»é‡ï¼šåŒä¸€é¡µå¯èƒ½è¢«åˆ‡åˆ†æˆå¤šä¸ª chunkï¼Œæˆ‘ä»¬åªæ˜¾ç¤ºå”¯ä¸€çš„é¡µç 
            unique_sources = sorted(list(set([m.get('page', '?') for m in metadatas])))
            
            citation_str = "\n\n---\n**ğŸ“š å‚è€ƒæ¥æº:**\n"
            # å¦‚æœæ¥æºå¤ªå¤šï¼Œåªæ˜¾ç¤ºå‰5ä¸ªé¡µç ï¼Œé¿å…å¤ªé•¿
            if len(unique_sources) > 5:
                pages_str = ", ".join([str(p) for p in unique_sources[:5]]) + "..."
            else:
                pages_str = ", ".join([str(p) for p in unique_sources])
                
            source_file = metadatas[0].get('source', 'Unknown')
            citation_str += f"- æ–‡ä»¶: {source_file}\n- é¡µç : {pages_str}\n"
            
            final_output = answer + citation_str
            # -----------------------------------------------

            # --- æ–°å¢ï¼šè‡ªåŠ¨ä¿å­˜å¯¹è¯æ—¥å¿— (æ–¹ä¾¿ AI åŠ©æ‰‹è¯»å–) ---
            try:
                log_path = os.path.join(os.path.dirname(CURRENT_DIR), "chat_history.log")
                with open(log_path, "a", encoding="utf-8") as f:
                    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    f.write(f"\n--- [{timestamp}] Type: {llm_type} ---\n")
                    f.write(f"Q: {query_text}\n")
                    f.write(f"A: {answer}\n")
                    f.write(f"Sources: Page {pages_str}\n")
                    f.write("-" * 30 + "\n")
            except Exception as log_err:
                print(f"å†™å…¥æ—¥å¿—å¤±è´¥: {log_err}")
            # -----------------------------------------------

            return final_output
        else:
            msg = "LLM æœªè¿”å›æœ‰æ•ˆç»“æœã€‚"
            print(msg)
            return msg
            
    except Exception as e:
        msg = f"è°ƒç”¨ LLM å¤±è´¥: {e}"
        print(msg)
        return msg

if __name__ == "__main__":
    # 1. æ„å»ºåº“ (å¦‚æœç¬¬ä¸€æ¬¡è¿è¡Œ)
    # æ³¨æ„ï¼šå¦‚æœåªæƒ³æµ‹è¯•æŸ¥è¯¢ï¼Œå¯ä»¥æ³¨é‡Šæ‰ build_vector_db
    # build_vector_db()
    
    # 2. RAG æµ‹è¯•
    print("\n" + "="*50)
    # ä½¿ç”¨ä¸­æ–‡æé—®ï¼Œæµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½ç”¨ä¸­æ–‡å›ç­”
    question = "Tesla 2023å¹´çš„æ€»æ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ" 
    rag_chat(question)
    
    print("\n" + "="*50)
    question2 = "è´¢æŠ¥ä¸­æåˆ°äº†å“ªäº›é£é™©å› ç´ ï¼Ÿ"
    rag_chat(question2)
