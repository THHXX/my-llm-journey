import os
import subprocess
import time
import requests
import psutil
import sys

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# é¡¹ç›®æ ¹ç›®å½• (ç¬¬43_48å‘¨)
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)

# è·¯å¾„é…ç½®
TOOLS_DIR = os.path.join(PROJECT_ROOT, "tools", "llama-cpp")
SERVER_EXE = os.path.join(TOOLS_DIR, "llama-server.exe")
MODELS_DIR = os.path.join(PROJECT_ROOT, "models")
MODEL_PATH = os.path.join(MODELS_DIR, "qwen1.5-0.5b-chat-q4_k_m.gguf")

HOST = "127.0.0.1"
PORT = 8080
API_URL = f"http://{HOST}:{PORT}/v1/chat/completions"

class LocalLLM:
    def __init__(self, port=8080):
        self.port = port
        self.process = None
        self.ensure_server_running()

    def is_port_in_use(self):
        """æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨"""
        for conn in psutil.net_connections():
            if conn.laddr.port == self.port:
                return True
        return False

    def ensure_server_running(self):
        """ç¡®ä¿ llama-server æ­£åœ¨è¿è¡Œ"""
        if self.is_port_in_use():
            print(f"   âœ… LLM æœåŠ¡ä¼¼ä¹å·²åœ¨ç«¯å£ {self.port} è¿è¡Œ")
            # å¯ä»¥åœ¨è¿™é‡Œåšä¸€ä¸ªç®€å•çš„å¥åº·æ£€æŸ¥
            return

        print(f"   ğŸš€ æ­£åœ¨å¯åŠ¨æœ¬åœ° LLM æœåŠ¡ (Port {self.port})...")
        print(f"   ğŸ“‚ æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
        
        if not os.path.exists(SERVER_EXE):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° llama-server.exe: {SERVER_EXE}")
        if not os.path.exists(MODEL_PATH):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}")

        # å¯åŠ¨å‘½ä»¤
        # å°è¯•ä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–åˆ‡æ¢å·¥ä½œç›®å½•ä»¥é¿å…ä¸­æ–‡è·¯å¾„é—®é¢˜
        model_filename = os.path.basename(MODEL_PATH)
        
        cmd = [
            SERVER_EXE,
            "-m", model_filename,
            "-c", "2048",
            "--host", HOST,
            "--port", str(self.port),
            "-ngl", "0" 
        ]

        # æ—¥å¿—æ–‡ä»¶
        log_file = open(os.path.join(PROJECT_ROOT, "llama_server.log"), "w")
        
        self.process = subprocess.Popen(
            cmd,
            cwd=MODELS_DIR, # åˆ‡æ¢å·¥ä½œç›®å½•åˆ°æ¨¡å‹æ‰€åœ¨ç›®å½•
            stdout=log_file,
            stderr=subprocess.STDOUT
        )
        
        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        print("   â³ ç­‰å¾…æœåŠ¡å°±ç»ª...", end="", flush=True)
        retries = 30
        for _ in range(retries):
            if self.is_port_in_use():
                print(" å®Œæˆ!")
                return
            time.sleep(1)
            print(".", end="", flush=True)
        
        raise RuntimeError("LLM æœåŠ¡å¯åŠ¨è¶…æ—¶")

    def chat(self, messages, temperature=0.7):
        """
        å‘é€èŠå¤©è¯·æ±‚
        messages: [{"role": "user", "content": "..."}]
        """
        payload = {
            "messages": messages,
            "temperature": temperature,
            "max_tokens": 1024,
            "frequency_penalty": 1.1, # å¢åŠ é‡å¤æƒ©ç½š
            "presence_penalty": 1.1,
            "top_p": 0.95
        }
        
        # é‡è¯•æœºåˆ¶
        max_retries = 5
        for attempt in range(max_retries):
            try:
                response = requests.post(API_URL, json=payload, timeout=60)
                
                # å¦‚æœæ˜¯ 503 (æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­)ï¼Œç­‰å¾…å¹¶é‡è¯•
                if response.status_code == 503:
                    print(f"   â³ æœåŠ¡ç¹å¿™æˆ–æ­£åœ¨åˆå§‹åŒ– (503)ï¼Œé‡è¯• {attempt+1}/{max_retries}...")
                    time.sleep(2)
                    continue
                    
                response.raise_for_status()
                return response.json()
            except requests.exceptions.RequestException as e:
                print(f"\nâŒ è¯·æ±‚ LLM å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    return None
        return None

    def stop(self):
        """åœæ­¢æœåŠ¡"""
        if self.process:
            self.process.terminate()
            self.process = None

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    try:
        llm = LocalLLM()
        
        test_msg = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}]
        print(f"\nğŸ—£ï¸  å‘é€æµ‹è¯•æ¶ˆæ¯: {test_msg[0]['content']}")
        
        result = llm.chat(test_msg)
        
        if result:
            content = result['choices'][0]['message']['content']
            print(f"\nğŸ¤– å›å¤:\n{content}")
        
        # ä¿æŒæœåŠ¡è¿è¡Œï¼Œæˆ–è€…é€‰æ‹© llm.stop()
        # llm.stop() 
        print("\nâœ… æµ‹è¯•å®Œæˆã€‚æœåŠ¡ä»åœ¨åå°è¿è¡Œï¼Œå¯ä¾› RAG ä½¿ç”¨ã€‚")
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
