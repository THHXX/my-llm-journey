import os
import gradio as gr
from dashscope import Generation
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from http import HTTPStatus

# ========== é…ç½®éƒ¨åˆ† ==========
# 1. è®¾ç½® HF é•œåƒï¼Œé˜²æ­¢æ¨¡å‹ä¸‹è½½è¶…æ—¶
# os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
if "HUGGINGFACE_SPACES" not in os.environ:
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# MODEL_PATH = r"C:\Users\JYJYJ\.cache\huggingface\hub\models--sentence-transformers--all-MiniLM-L6-v2\snapshots\c9745ed1d9f207416be6d2e6f8de32d1f16199bf"
# æ›¿æ¢ä¸ºä¸­æ–‡æ•ˆæœæ›´å¥½çš„ Embedding æ¨¡å‹
# ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆçº¦ 100MBï¼‰ï¼Œè¯·ç¡®ä¿ç½‘ç»œé€šç•…
MODEL_PATH = "BAAI/bge-small-zh-v1.5"
# MODEL_PATH = r"C:\Users\JYJYJ\.cache\huggingface\hub\models--BAAI--bge-small-zh-v1.5\snapshots\7999e1d3359715c523056ef9478215996d62a620"
# å…¨å±€å˜é‡ï¼šç”¨äºå­˜å‚¨æ„å»ºå¥½çš„å‘é‡åº“
global_vectorstore = None

# ========== æ ¸å¿ƒé€»è¾‘ ==========

def load_and_split_files(file_objs):
    """
    åŠ è½½å¹¶åˆ‡åˆ†ä¸Šä¼ çš„æ–‡ä»¶ã€‚
    æ”¯æŒ PDF å’Œ TXTã€‚
    """
    all_chunks = []
    
    if not file_objs:
        return []

    for file_obj in file_objs:
        # Gradio 4.x ä¼ å…¥çš„æ˜¯æ–‡ä»¶å¯¹è±¡åˆ—è¡¨ï¼Œfile_obj.name æ˜¯ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        file_path = file_obj.name
        filename = os.path.basename(file_path)
        print(f"[System] æ­£åœ¨å¤„ç†æ–‡ä»¶: {filename}")
        
        # æ ¹æ®åç¼€é€‰æ‹©åŠ è½½å™¨
        if filename.lower().endswith(".pdf"):
            loader = PyPDFLoader(file_path)
        elif filename.lower().endswith(".txt"):
            loader = TextLoader(file_path, encoding="utf-8", autodetect_encoding=True)
        else:
            print(f"[System] è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}")
            continue

        docs = loader.load()
        
        # åˆ‡åˆ†æ–‡æœ¬
        # é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–åˆ‡åˆ†å‚æ•°
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,  # å‡å°åˆ‡åˆ†ç²’åº¦ï¼Œæé«˜æ£€ç´¢ç²¾å‡†åº¦
            chunk_overlap=50 # å‡å°‘é‡å 
        )
        chunks = splitter.split_documents(docs)
        all_chunks.extend(chunks)
        
    return all_chunks

def build_vectorstore(file_objs):
    """
    æ ¹æ®ä¸Šä¼ çš„æ–‡ä»¶æ„å»ºå‘é‡åº“ã€‚
    """
    global global_vectorstore
    
    chunks = load_and_split_files(file_objs)
    if not chunks:
        return "âš ï¸ æœªæå–åˆ°æœ‰æ•ˆæ–‡æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚"

    print(f"[System] å…±åˆ‡åˆ†å‡º {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œå¼€å§‹æ„å»ºå‘é‡åº“...")
    
    # åˆå§‹åŒ– Embedding æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_PATH)
    
    # æ„å»º FAISS ç´¢å¼•
    global_vectorstore = FAISS.from_documents(chunks, embeddings)
    
    return f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼å…±å¤„ç† {len(chunks)} ä¸ªç‰‡æ®µã€‚ç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†ã€‚"

def qwen_chat(message, history):
    """
    å¤„ç†ç”¨æˆ·æé—®ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰ã€‚
    message: å½“å‰ç”¨æˆ·è¾“å…¥
    history: å†å²å¯¹è¯åˆ—è¡¨ [[user_msg, bot_msg], ...]
    è¿”å›: ç”Ÿæˆçš„å›ç­”æ–‡æœ¬
    """
    global global_vectorstore
    
    if not os.environ.get("DASHSCOPE_API_KEY"):
        return "âŒ è¯·å…ˆè®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡ã€‚"
    
    print(f"[DEBUG] message type: {type(message)}")
    print(f"[DEBUG] message content: {message}")

    # é˜²å¾¡æ€§å¤„ç†ï¼šå¦‚æœ message æ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
    if isinstance(message, list):
        if len(message) > 0:
            message = message[0]
            print(f"[DEBUG] Converted list message to: {message}")
        else:
            message = ""
    
    # ç¡®ä¿ message æ˜¯å­—ç¬¦ä¸²
    if not isinstance(message, str):
        message = str(message)

    if global_vectorstore is None:
        return "âš ï¸ è¯·å…ˆä¸Šä¼ æ–‡ä»¶å¹¶ç­‰å¾…çŸ¥è¯†åº“æ„å»ºå®Œæˆã€‚"

    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    try:
        # å¢åŠ æ£€ç´¢æ•°é‡ k=5ï¼Œæä¾›æ›´å¤šä¸Šä¸‹æ–‡
        retrieved_docs = global_vectorstore.similarity_search(message, k=5)
        context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    except Exception as e:
        return f"æ£€ç´¢å‡ºé”™: {str(e)}"

    # 2. æ„å»º Prompt
    history_str = ""
    # å†å²è®°å½•æ ¼å¼ä¸º [{'role': 'user', 'content': '...'}, {'role': 'assistant', 'content': '...'}]
    # å–æœ€è¿‘ 4 æ¡æ¶ˆæ¯ï¼ˆ2è½®å¯¹è¯ï¼‰
    recent_history = history[-4:] if len(history) >= 4 else history
    for msg in recent_history:
        role = "ç”¨æˆ·" if msg['role'] == "user" else "åŠ©æ‰‹"
        history_str += f"{role}: {msg['content']}\n"

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘
{context}

ã€å†å²å¯¹è¯ã€‘
{history_str}

ã€ç”¨æˆ·å½“å‰é—®é¢˜ã€‘
{message}

è¯·æ³¨æ„ï¼š
1. è¯·åŠ¡å¿…ä»…ä¾æ®ã€å‚è€ƒèµ„æ–™ã€‘ä¸­çš„å†…å®¹å›ç­”ï¼Œä¸è¦ä½¿ç”¨ä½ è‡ªå·±çš„å¤–éƒ¨çŸ¥è¯†ã€‚
2. å¦‚æœã€å‚è€ƒèµ„æ–™ã€‘ä¸­æ²¡æœ‰åŒ…å«é—®é¢˜çš„ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´â€œæŠ±æ­‰ï¼Œæ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜â€ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
3. å›ç­”è¦æ¡ç†æ¸…æ™°ï¼Œä½¿ç”¨ä¸­æ–‡ï¼Œå¹¶å°½é‡å¼•ç”¨èµ„æ–™ä¸­çš„åŸæ–‡ã€‚
4. è¯·ä¸€æ­¥æ­¥æ€è€ƒï¼Œç¡®ä¿é€»è¾‘ä¸¥å¯†ã€‚
"""

    # 3. è°ƒç”¨ Qwen API
    try:
        response = Generation.call(
            model="qwen-max",
            api_key=os.environ.get("DASHSCOPE_API_KEY"),
            prompt=prompt
        )
        
        if response.status_code == HTTPStatus.OK:
            return response.output.text
        else:
            return f"API è°ƒç”¨å¤±è´¥: {response.message}"
            
    except Exception as e:
        return f"å‘ç”Ÿé”™è¯¯: {str(e)}"

def user_input(user_msg, history):
    """ç”¨æˆ·è¾“å…¥å¤„ç†ï¼šå°†æ¶ˆæ¯æ·»åŠ åˆ°å†å²è®°å½•ï¼Œå¹¶æ¸…ç©ºè¾“å…¥æ¡†"""
    if history is None:
        history = []
    # å¼ºåˆ¶ä½¿ç”¨ OpenAI æ ¼å¼ï¼Œæ»¡è¶³ Gradio æŠ¥é”™è¦æ±‚
    return "", history + [{"role": "user", "content": user_msg}]

def bot_response(history):
    """æœºå™¨äººå“åº”å¤„ç†ï¼šè·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œè°ƒç”¨æ¨¡å‹ï¼Œæ›´æ–°å†å²è®°å½•"""
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„å†…å®¹
    user_msg = history[-1]['content']
    # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”ï¼Œä¼ å…¥é™¤å½“å‰é—®é¢˜å¤–çš„å†å²è®°å½•
    bot_msg = qwen_chat(user_msg, history[:-1])
    # æ·»åŠ æœºå™¨äººå›ç­”
    history.append({"role": "assistant", "content": bot_msg})
    return history

# ========== Gradio ç•Œé¢ ==========

with gr.Blocks(title="ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹") as demo:
    gr.Markdown("# ğŸ“š ä¸ªäºº RAG çŸ¥è¯†åº“åŠ©æ‰‹")
    gr.Markdown("æ”¯æŒä¸Šä¼  PDF/TXT æ–‡ä»¶ï¼Œæ„å»ºä¸“å±çŸ¥è¯†åº“å¹¶è¿›è¡Œé—®ç­”ã€‚")
    
    with gr.Row():
        with gr.Column(scale=1):
            # æ–‡ä»¶ä¸Šä¼ åŒº
            file_input = gr.File(
                label="1. ä¸Šä¼ æ–‡æ¡£ (æ”¯æŒå¤šé€‰)", 
                file_count="multiple",
                file_types=[".pdf", ".txt"]
            )
            upload_btn = gr.Button("ğŸš€ æ„å»ºçŸ¥è¯†åº“", variant="primary")
            status_output = gr.Textbox(label="ç³»ç»ŸçŠ¶æ€", interactive=False)
            
        with gr.Column(scale=2):
            # èŠå¤©åŒº
            # ä¸ä¼  type å‚æ•°ï¼Œä½†æ•°æ®æ ¼å¼æ”¹ä¸ºå­—å…¸åˆ—è¡¨ï¼Œä»¥æ»¡è¶³æŠ¥é”™æç¤ºçš„è¦æ±‚
            chatbot = gr.Chatbot(height=500, label="å¯¹è¯è®°å½•")
            msg_input = gr.Textbox(label="2. è¾“å…¥é—®é¢˜", placeholder="å…³äºæ–‡æ¡£å†…å®¹çš„æé—®...")
            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯")

    # äº‹ä»¶ç»‘å®š
    upload_btn.click(
        fn=build_vectorstore,
        inputs=[file_input],
        outputs=[status_output]
    )
    
    # æäº¤é—®é¢˜åçš„å¤„ç†æµç¨‹ï¼š
    # 1. user_input: æ›´æ–° Chatbot æ˜¾ç¤ºç”¨æˆ·é—®é¢˜ï¼Œæ¸…ç©ºè¾“å…¥æ¡†
    # 2. bot_response: è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”ï¼Œæ›´æ–° Chatbot æ˜¾ç¤ºåŠ©æ‰‹å›ç­”
    msg_input.submit(
        fn=user_input,
        inputs=[msg_input, chatbot],
        outputs=[msg_input, chatbot]
    ).then(
        fn=bot_response,
        inputs=[chatbot],
        outputs=[chatbot]
    )
    
    # æ¸…ç©ºæŒ‰é’®
    clear_btn.click(lambda: None, None, chatbot, queue=False)


if __name__ == "__main__":
    demo.launch()