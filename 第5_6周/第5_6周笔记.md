## 第5–6周学习笔记：从 PDF 到 RAG 问答系统

### 一、整体目标回顾

- **本周大目标**：  
  搭建一个「**基于本地 PDF 的问答系统**」——你给系统一篇 PDF，问它「这篇文章讲了什么？」，它能基于文章内容，给出比较准确的**中文摘要**。

- **你实际已经完成的链路**：
  1. **加载 PDF → 文档对象 `Document` 列表**
  2. **文本切分 → 很多小块 `Document`**
  3. **embedding + FAISS → 向量库（可相似检索）**
  4. **检索相关块 + 调 Qwen → 生成摘要回答（完整 RAG）**

这已经是一个**真正可用的迷你 RAG 系统**了。

---

### 二、核心概念理解

#### 1. RAG：Retrieval-Augmented Generation

- **全称**：Retrieval-Augmented Generation（检索增强生成）。
- **核心思想**：
  - **先检索**：从「外部知识库」中找到和问题最相关的一小部分文本；
  - **再生成**：让大模型在看了这些文本的基础上生成答案。
- **好处**：
  - 不再让大模型“瞎猜”，而是「有根有据」；
  - 可以随时换文档，而不需要重新训练大模型。

你这次的系统中：

- 「外部知识库」= 你那篇 PDF 切成的很多文本块组成的向量库；
- 「生成模型」= 通义千问 Qwen（`qwen-max`）。

---

#### 2. LangChain：帮你「搭流水线」的框架

- **LangChain 是什么？**
  - 一个专门为大模型应用设计的「**拼装框架**」，帮助你把：
    - 文档加载
    - 文本切分
    - 向量化（embedding）
    - 向量搜索（FAISS）
    - 调用大模型生成答案  
    这些步骤像搭积木一样串起来。

- 在你的项目里，它负责：
  - **文档加载**：`PyPDFLoader` 从 PDF 读成 `Document` 列表；
  - **文本切分**：`RecursiveCharacterTextSplitter` 把长文本切成块；
  - **向量库**：`FAISS.from_documents` 构建可检索的向量库。

---

#### 3. Embedding / sentence-transformers / FAISS

- **Embedding（向量表示）**：
  - 把一句话 / 一段文本 → 一个有很多维度的数字向量（例如 384 维）；
  - 向量之间的「距离」可以表示语义相似度。

- **sentence-transformers**：
  - 一个专门做「句向量」的 Python 库；
  - 你用的是模型：`sentence-transformers/all-MiniLM-L6-v2`：
    - 体积小（几十 MB 左右），
    - 参数量几千万，
    - 输出 384 维向量，
    - 适合本地实验。

- **FAISS**：
  - Facebook 开源的「**向量相似度搜索库**」；
  - 用来：
    - 建立一个向量索引（Vector Store），
    - 接收一条查询向量，快速找到「最近的几个向量」以及对应的原始文本。

在你的 RAG 系统里：

- 每个 PDF 文本块 → `HuggingFaceEmbeddings` → 向量；
- 所有向量 + 文本块 → `FAISS.from_documents` → 向量库；
- 问题 → embedding → FAISS → 找出最相似的 k 个块。

---

#### 4. Qwen（通义千问）在这里扮演的角色

- **Qwen 是生成式大模型（LLM）**：
  - 负责「看检索到的文本块 + 用户问题」→ 输出自然语言摘要。
- 你使用的是 **DashScope 的 Python SDK**：
  - 通过 `Generation.call(model="qwen-max", api_key=..., prompt=...)` 调用；
  - `prompt` 里既包含了**检索到的原文片段**，也包含了**你的问题和回答要求**。

RAG 的分工可以这样记：

- **Embedding + FAISS**：帮你找到「文章里最相关的几段话」；
- **Qwen**：负责「读完这几段，帮你总结成一段更好理解的摘要」。

---

### 三、你写的 RAG 代码结构梳理（重点）

文件：`第5_6周/rag_qa.py`

#### 1. 文档加载与切分

- **加载 PDF**（知识来源）：

  ```python
  def load_docs_from_pdf() -> list:
      """使用 PyPDFLoader 从本地 PDF 文件加载文档。"""
      pdf_path = "./第5_6周/1.pdf"  # 你的 PDF 路径
      loader = PyPDFLoader(pdf_path)  # 创建 PDF 加载器
      docs = loader.load()  # 读取 PDF，返回一个 Document 列表
      return docs
  ```

- **文本切分**（把大块拆成小块）：

  ```python
  def split_docs(docs: list) -> list:
      """使用递归字符切分器，把长文档切成很多小块。"""
      text_splitter = RecursiveCharacterTextSplitter(
          chunk_size=500,      # 每块大约 500 个字符
          chunk_overlap=100,   # 块与块之间重叠 100 字符，保持上下文连续
      )
      chunks = text_splitter.split_documents(docs)
      return chunks
  ```

**为什么要切块？**

- 整篇文章太长，大模型一次吃不下；
- 检索时，以「块」为单位更细粒度，可以更精准地找到相关上下文；
- 设置重叠是为了避免「句子刚好被切断，信息被分在两块中」。

---

#### 2. 构建向量库（Embedding + FAISS）

```python
def build_vectorstore(chunks: list):
    """
    使用 HuggingFaceEmbeddings + FAISS 构建向量库。
    """
    model_name = "sentence-transformers/all-MiniLM-L6-v2"  # 第4周用过的模型
    embeddings = HuggingFaceEmbeddings(model_name=model_name)  # 本地向量模型

    vectorstore = FAISS.from_documents(chunks, embeddings)  # 文本块 → 向量库
    return vectorstore
```

**这一步你做了什么？**

- 选定一个 Embedding 模型（`all-MiniLM-L6-v2`）；
- 用这个模型把每个文本块编码成向量；
- 把所有向量 + 文本块交给 FAISS，建立可以相似度搜索的索引。

---

#### 3. 检索测试（只检索，不生成）

```python
def test_similarity_search(vectorstore):
    """
    用一条示例问题，在向量库里检索最相似的文档块。
    目的：检查切分 + 向量化是否合理。
    """
    query = "这篇文章主要讲了什么？"
    k = 3

    results = vectorstore.similarity_search(query, k=k)

    print(f"检索问题: {query}")
    print(f"返回的文档块数量: {len(results)}")
    for i, doc in enumerate(results):
        print("=" * 40)
        print(f"第 {i + 1} 个相似块：")
        print(doc.page_content[:300])
```

**这一步的意义**：

- 不直接让 LLM 生成，而是先检查：  
  - Embedding + FAISS 找出来的块，人眼看是不是合理；
  - 如果检索出来的是和问题很不相干的内容，那说明前面的切分/embedding 需要调整。

---

#### 4. Qwen 调用封装（Generation）

```python
def run_qwen_generation(prompt: str, api_key: str) -> Optional[str]:
    """
    调用通义千问 Qwen（dashscope）生成答案的封装函数。
    """
    try:
        response = Generation.call(
            model="qwen-max",
            api_key=api_key,
            prompt=prompt,
        )
    except Exception as exc:
        print(f"调用 Qwen 异常: {exc}", file=sys.stderr)
        return None

    if response.status_code == HTTPStatus.OK:
        return response.output.text

    print(
        f"调用 Qwen 失败，状态码: {response.status_code}, 错误信息: {response.message}",
        file=sys.stderr,
    )
    return None
```

**这里你做了 3 件事**：

- 用统一函数封装了 Qwen 调用逻辑；
- 对异常和错误状态码做了处理（不会一崩就看不懂原因）；
- 让上层调用只关心「给 prompt → 得到文本」。

---

#### 5. RAG 问答函数：检索 + 生成合体

```python
def answer_question_with_rag(vectorstore, question: str) -> None:
    """
    使用 RAG 流程回答问题：
    1. 先用向量库检索与问题最相关的文档块（Retrieval）
    2. 再把这些文档块作为上下文，交给 Qwen 生成答案（Generation）
    """
    # 1. 检索
    k = 4
    retrieved_docs = vectorstore.similarity_search(question, k=k)

    # 2. 拼接上下文
    context = "\n\n------ 文档块分隔线 ------\n\n".join(
        doc.page_content for doc in retrieved_docs
    )

    # 3. 构造 Prompt
    prompt = f"""
你是一个严谨的阅读理解助手。请严格按照下面的要求回答问题。

【已检索到的文章片段】
{context}

【用户问题】
{question}

【回答要求】
1. 你的回答必须只基于【已检索到的文章片段】中的信息，不要自己编造内容。
2. 请用中文给出这篇文章的简要摘要，控制在 100~150 字左右。
3. 如果片段中的信息不足以回答问题，请明确说明“根据当前片段信息不足，无法给出完整答案”。
""".strip()

    # 4. 读取 Qwen API Key
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        print(
            "未找到环境变量 DASHSCOPE_API_KEY，请先配置你的 Qwen API Key。",
            file=sys.stderr,
        )
        return

    # 5. 调用 Qwen
    answer = run_qwen_generation(prompt, api_key)
    if answer is None:
        print("Qwen 调用失败，无法生成答案。", file=sys.stderr)
        return

    # 6. 打印最终回答
    print("\n=== 基于 RAG 的 Qwen 回答 ===")
    print(answer)
```

**几个关键点：**

- **检索和生成彻底分离**：
  - `vectorstore.similarity_search` 只负责找「相关文本块」；
  - Qwen 只负责基于这些文本块来「总结和回答」。
- **Prompt 里强约束**：
  - 只能依据【已检索到的文章片段】回答；
  - 要求输出**摘要**，限定字数范围；
  - 信息不够要说「不足」，避免乱编。
- **RAG 的本质**就在这一个函数里完整体现了。

---

#### 6. 主流程 `main()`：把所有步骤串起来

```python
def main():
    """加载 PDF -> 切分 -> 构建向量库 -> 做一次检索测试 -> 调用 Qwen 回答。"""
    docs = load_docs_from_pdf()
    print(f"原始文档数量: {len(docs)}")

    chunks = split_docs(docs)
    print(f"切分后的文档块数量: {len(chunks)}")

    vectorstore = build_vectorstore(chunks)

    test_similarity_search(vectorstore)  # 检查检索结果是否合理

    question = "这篇文章主要讲了什么？"
    answer_question_with_rag(vectorstore, question)  # 真正的RAG问答
```

**这就是完整的一条 RAG「流水线」**。

---

### 四、本周踩过的坑 & 学到的实战经验

- **PowerShell 命令习惯**：
  - `&&` 不能开头单独用；要么写成 `cmd1; cmd2`，要么整行是一个完整命令；
  - 激活虚拟环境、切目录时要注意路径中的中文和反斜杠。

- **Hugging Face 下载问题**：
  - 一开始连不上 `huggingface.co`，模型下不下来；
  - 换网络 / 重试后看到进度条 `model.safetensors 90.9M`，才算真正完成下载；
  - 模型缓存默认在 `C:\Users\你的用户名\.cache\huggingface\...` 下，下完后下次就不会再下载。

- **LangChain 版本变化**：
  - 新版不再用 `from langchain.text_splitter import ...`，而是 `langchain_text_splitters` 独立包；
  - 实战中遇到 `ModuleNotFoundError`，学会了：
    - 看错误提示里的模块名；
    - 去安装 `langchain-text-splitters`；
    - 修改 import 路径。

- **Embedding 相似度的理解**：
  - 第 4 周用 `all-MiniLM-L6-v2` 时，相似句子相似度约 0.65；
  - 明白了「0.7 是经验阈值，不是绝对标准」，要结合具体任务和模型来判断。

---

### 五、你可以继续思考/尝试的方向（选做）

- **改问题再试**：
  - 把问题改成「这篇文章主要讨论了哪几个方面？」、  
    「这篇文章提到了哪些具体数据或案例？」  
  看看 RAG+Qwen 的表现有什么不同。

- **增加「显示原文片段」的选项**：
  - 在最终回答前，再打印一次被检索到的文档块（你已经做过类似的调试输出），  
  方便对比「模型有没有踩着原文说话」。

- **尝试改 Prompt 约束**：
  - 比如加上「请分条列出 3 个关键点」；
  - 或者加一句「先给出 1 句话的总总结，再给出 3 条详细要点」。

---
