# 第40周操作指南：模型量化 (GGUF + AWQ)

本周我们将深入大模型“瘦身”技术。你将体验两种主流量化方案：
1.  **GGUF (llama.cpp)**：CPU 推理的神器，适合在普通笔记本上运行。
2.  **AWQ (vLLM)**：GPU 推理的高效方案，适合服务器部署。

> **⚠️ 注意**：`vLLM` 对 Windows 支持尚处于实验阶段，且不支持 GGUF 格式。如果你的电脑是核显或 `vLLM` 安装失败，请重点完成第一部分 (GGUF)。

## 1. 准备工作

### 1.1 复用已有的 llama.cpp 工具
**好消息！** 检测到你在 [第20_21周](../第20_21周/) 已经下载过 `llama.cpp` 的编译文件。
我已经自动帮你把它们复制到了本周的目录中，你**不需要重新下载**。

- 工具路径: `F:\Desktop\个人资料\大模型学习项目\第40周\llama.cpp`
- 转换脚本: `F:\Desktop\个人资料\大模型学习项目\第40周\convert_hf_to_gguf.py`

### 1.2 安装依赖
在终端中运行：
```powershell
pip install modelscope torch transformers autoawq vllm
```
*注：`vllm` 和 `autoawq` 可能在 Windows 上安装困难。如果报错，可跳过 `vLLM` 部分，专注于 `llama.cpp`。*

## 2. 步骤一：模型准备 (已完成)

**好消息！** 检测到你已经拥有了转换好的 GGUF 模型：
- FP16 (未量化): `model\qwen-0.5b-fp16.gguf`
- Q4_K_M (已量化): `model\qwen-0.5b-q4_k_m.gguf`

我已经帮你把它们复制到了 `第40周\model` 目录。
你可以直接跳过下载和转换步骤，直接开始**量化实操**（如果你想练习）或者**直接运行**。

## 3. 步骤二：GGUF 量化与测试 (llama.cpp)

### 3.1 练习量化 (可选)
虽然你已经有了量化后的模型 `qwen-0.5b-q4_k_m.gguf`，但我建议你删除它并重新跑一次量化命令，亲手体验模型“变小”的过程：

```powershell
# 进入 llama.cpp 目录
cd llama.cpp

# 运行量化命令 (将 FP16 压缩为 4-bit)
# 注意：新版本工具名为 llama-quantize.exe
.\llama-quantize.exe ..\model\qwen-0.5b-fp16.gguf ..\model\my_quantized_q4.gguf Q4_K_M
```
*注：生成的 `my_quantized_q4.gguf` 应该和原来的 `qwen-0.5b-q4_k_m.gguf` 大小一致。*

### 3.2 运行推理测试
现在，让我们看看量化后的模型跑得有多快！

**使用你现有的量化模型运行：**
```powershell
# 注意：新版本工具名为 llama-cli.exe (原 main.exe)
.\llama-cli.exe -m ..\model\qwen-0.5b-q4_k_m.gguf -p "为什么天空是蓝色的？" -n 128 -c 512
```

**对比测试 (使用未量化的 FP16 模型)：**
```powershell
.\llama-cli.exe -m ..\model\qwen-0.5b-fp16.gguf -p "为什么天空是蓝色的？" -n 128 -c 512
```
*观察两者的 `eval time` (推理耗时) 和内存占用区别。*

## 4. 步骤三：vLLM 加载 (AWQ)
> **知识点**：`vLLM` 不支持 GGUF 格式，它主要支持 AWQ 或 GPTQ 量化。

如果你成功安装了 `vLLM` (通常需要 Linux/WSL2)，可以运行以下脚本体验：

运行 `vllm_test.py`:
```powershell
python vllm_test.py
```

## 5. 验收标准
1.  查看 `model` 目录，比较 `qwen-0.5b-fp16.gguf` (约 1GB) 和 `qwen-0.5b-q4_k_m.gguf` (约 0.4GB) 的大小。应减少 50% 以上。
2.  `main.exe` 运行时的生成速度飞快。
