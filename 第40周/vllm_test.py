import os
import time
import sys

# -------------------------------------------------------------------------
# ç”¨æˆ·æŒ‡å®šæ¨¡å‹è·¯å¾„ (Windows æ ¼å¼)
# -------------------------------------------------------------------------
ORIGIN_MODEL_PATH = r"C:\Users\JYJYJ\.cache\huggingface\hub\models--qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39"

def get_wsl_path(win_path):
    """å¦‚æœæ˜¯åœ¨ WSL2 ç¯å¢ƒä¸‹ï¼Œè‡ªåŠ¨å°† Windows è·¯å¾„è½¬æ¢ä¸º Linux è·¯å¾„"""
    if sys.platform.startswith('linux'):
        # ç®€å•åˆ¤æ–­æ˜¯å¦ä¸º C ç›˜è·¯å¾„å¹¶è½¬æ¢
        if "C:" in win_path:
            return win_path.replace("C:", "/mnt/c").replace("\\", "/")
    return win_path

# è·å–æœ€ç»ˆè·¯å¾„
model_path = get_wsl_path(ORIGIN_MODEL_PATH)

# -------------------------------------------------------------------------
# vLLM æ£€æµ‹ä¸åŠ è½½
# -------------------------------------------------------------------------
try:
    from vllm import LLM, SamplingParams
    VLLM_INSTALLED = True
except ImportError:
    VLLM_INSTALLED = False
    print("âŒ æœªæ£€æµ‹åˆ° vLLM åº“ã€‚")
    print("åŸå› ï¼šå½“å‰æ˜¯ Windows åŸç”Ÿç¯å¢ƒï¼Œrequirements.txt å·²è‡ªåŠ¨è·³è¿‡å®‰è£…ã€‚")
    print("--------------------------------------------------")
    print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ (äºŒé€‰ä¸€)ï¼š")
    print("1. [æ¨è] æ”¾å¼ƒ vLLMï¼Œæ”¹ç”¨ llama.cpp (GGUF) è¿è¡Œé‡åŒ–æ¨¡å‹ã€‚")
    print("2. [æ­»ç£•] è¯·å®‰è£… WSL2 (Ubuntu)ï¼Œåœ¨ Linux ç¯å¢ƒä¸­è¿è¡Œæ­¤ä»£ç ã€‚")
    print("--------------------------------------------------")

if VLLM_INSTALLED:
    print(f"ğŸš€ æ­£åœ¨ä½¿ç”¨ vLLM åŠ è½½æ¨¡å‹: {model_path}")
    print("æ³¨æ„ï¼šæ˜¾å­˜éœ€å¤§äº 4GBï¼Œå¦åˆ™è¯·è°ƒä½ gpu_memory_utilization")

    try:
        # åˆå§‹åŒ– vLLM (gpu_memory_utilization æ§åˆ¶æ˜¾å­˜å ç”¨ï¼Œ0.6 è¡¨ç¤ºå ç”¨ 60%)
        llm = LLM(model=model_path, quantization=None, gpu_memory_utilization=0.6)
        
        # å®šä¹‰é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=128)

        # æµ‹è¯• Prompts
        prompts = [
            "Hello, my name is",
            "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ",
        ]

        print("âš¡ å¼€å§‹æ¨ç†...")
        start_time = time.time()
        
        outputs = llm.generate(prompts, sampling_params)
        
        end_time = time.time()

        # è¾“å‡ºç»“æœ
        for output in outputs:
            prompt = output.prompt
            generated_text = output.outputs[0].text
            print(f"\nPrompt: {prompt}\nGenerated: {generated_text}")
            
        print(f"\nâœ… æ¨ç†å®Œæˆï¼è€—æ—¶: {end_time - start_time:.2f} ç§’")
        
    except Exception as e:
        print(f"âš ï¸ vLLM è¿è¡Œå‡ºé”™: {e}")