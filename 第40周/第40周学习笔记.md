# 第40周实战笔记：从 vLLM 挫折到 GGUF 胜利

> **摘要**：本周经历了从 Windows 环境配置 vLLM 失败，到成功迁移至 WSL (Linux)，并最终通过 llama.cpp 实现模型量化与运行的完整过程。这是一次从“工具使用者”向“开发者”思维的转变。

[TOC]

---

## 1. 核心结论：两条路线的选择

在 Windows 电脑上运行大模型，根据硬件配置不同，有两条截然不同的路线：

| 特性 | **路线 A: vLLM (工业级)** | **路线 B: llama.cpp (通用级)** |
| :--- | :--- | :--- |
| **适用硬件** | **NVIDIA 独显** (必须支持 CUDA) | **任意硬件** (CPU / AMD / 核显 / N卡) |
| **适用系统** | Linux / WSL2 (Windows原生支持极差) | Windows / Linux / macOS |
| **核心优势** | 极高的并发吞吐量 (适合服务器) | **兼容性无敌**，低资源消耗 (适合个人电脑) |
| **本周结果** | ❌ **失败** (因硬件为核显/驱动限制) | ✅ **成功** (完美运行 Q4 量化模型) |

**💡 经验总结**：
- 如果没有 NVIDIA 显卡，**不要死磕 vLLM**，直接拥抱 **llama.cpp (GGUF)**。
- **WSL (Windows Subsystem for Linux)** 是大模型开发的必备环境，不管用哪条路线，先装好 WSL 准没错。

---

## 2. 环境升级：拥抱 WSL (Linux)

为了解决 Windows 下各种依赖报错和 C 盘爆满的问题，我们完成了架构升级。

### 2.1 为什么要用 WSL？
1.  **原生 Linux 体验**：大模型开源项目 99% 都是优先支持 Linux 的。
2.  **文件隔离**：可以将整个 Linux 系统装在 **D 盘** (如 `D:\WSL_Ubuntu`)，彻底解放 C 盘。
3.  **极速下载**：配合国内镜像源，pip 和 apt 的速度飞快。

### 2.2 关键操作复盘
- **迁移安装**：将 WSL 安装包移动到 D 盘安装，避免占用 C 盘空间。
- **连接 VS Code**：使用 VS Code 的 `Remote - WSL` 插件，直接在 Windows 上编辑和运行 Linux 里的代码。

---

## 3. 全流程复盘：GGUF 量化实战

这是本周的**MVP (Most Valuable Process)**，一套标准化的“炼丹”流程。

### 3.1 准备工作 (WSL 终端)
```bash
# 1. 更新软件源
sudo apt update

# 2. 安装编译工具 (cmake, gcc) 和 Python 基础
sudo apt install cmake build-essential python3-pip libcurl4-openssl-dev -y

# 3. 重新编译 llama.cpp (确保是最新的)
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
cmake -B build
cmake --build build --config Release -j $(nproc)  # -j 调用多核加速编译
```

### 3.2 模型下载 (国内镜像加速)
使用 `huggingface-cli` 配合国内镜像，跑满宽带。

```bash
# 1. 开启镜像加速 (临时环境变量)
export HF_ENDPOINT=https://hf-mirror.com

# 2. 安装下载工具
pip install huggingface_hub

# 3. 下载原始模型 (Qwen1.5-0.5B)
# --local-dir 指定下载到当前目录，防止下到 C 盘
huggingface-cli download Qwen/Qwen1.5-0.5B --local-dir models/Qwen1.5-0.5B --local-dir-use-symlinks False --resume-download
```

### 3.3 格式转换 (HuggingFace -> GGUF FP16)
这一步将模型的数据结构转换为 llama.cpp 能识别的 GGUF 格式，但此时还未压缩 (FP16)。

```bash
# 安装转换脚本依赖
pip install -r requirements.txt

# 运行转换脚本
python3 convert_hf_to_gguf.py models/Qwen1.5-0.5B --outfile models/qwen1.5-0.5b-fp16.gguf
```

### 3.4 极致量化 (FP16 -> Q4_K_M)
使用 C++ 编译好的工具，进行数学运算，将 16 位浮点数压缩为 4 位整数。

```bash
# 语法: ./llama-quantize [输入文件] [输出文件] [量化方法]
./build/bin/llama-quantize models/qwen1.5-0.5b-fp16.gguf models/qwen1.5-0.5b-q4_k_m.gguf Q4_K_M
```

### 3.5 最终运行 (推理)
见证奇迹的时刻，加载量化后的模型进行对话。

```bash
# -m 指定模型, -p 提示词, -n 最大字数, --color 开启彩色
./build/bin/llama-cli -m models/qwen1.5-0.5b-q4_k_m.gguf -p "你好，请介绍一下你自己" -n 128 --color on
```

---

## 4. 关键指标对比 (Before vs After)

通过实战，我们验证了量化的巨大优势：

| 指标 | 原始模型 (FP16) | 量化模型 (Q4_K_M) | 变化幅度 |
| :--- | :--- | :--- | :--- |
| **文件大小** | ~1.2 GB | **~380 MB** | **缩小 68%** |
| **显存/内存需求** | > 2 GB | **< 1 GB** | 大幅降低 |
| **推理速度** | 较慢 | **飞快 (60+ token/s)** | 显著提升 |
| **智能程度** | 100% | ~95% (几乎无感) | 极小损失 |

---

## 5. 避坑指南 (血泪教训)

1.  **C 盘空间**：千万不要在 Windows PowerShell 直接跑下载命令！默认都会塞满 `C:\Users\.cache`。**一定要在 WSL 里，并指定下载路径**。
2.  **vLLM 兼容性**：AMD/Intel 核显用户直接放弃 vLLM，不要浪费时间报错，转身拥抱 llama.cpp。
3.  **编译报错**：如果 `cmake` 报错缺 `CURL`，记得安装 `libcurl4-openssl-dev`。
4.  **权限问题**：Linux 下操作系统文件需要 `sudo`，但操作自己项目目录下的文件尽量不要用 `sudo`，以免权限混乱。

---

> **总结**：第 40 周不仅学会了量化，更重要的是**构建了稳定的 Linux 开发环境 (WSL)**。这套环境和方法论，是未来处理更大模型（如 7B, 70B）的基石。
