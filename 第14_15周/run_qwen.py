import os
from modelscope import AutoModelForCausalLM, AutoTokenizer

# 1. æç®€ä¸»ä¹‰ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢ä¸å¿…è¦çš„è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

def run_inference():
    print("ğŸš€ æ­£åœ¨åŠ è½½ Qwen-0.5B æ¨¡å‹ (ä» ModelScope å›½å†…æº)...")
    
    # 2. åŠ è½½åˆ†è¯å™¨ (Tokenizer) - è´Ÿè´£æŠŠäººè¯å˜æˆæ•°å­—
    # trust_remote_code=True æ˜¯å¿…é¡»çš„ï¼Œå› ä¸º Qwen çš„ä»£ç åœ¨è¿œç¨‹ä»“åº“é‡Œ
    # ä¿®å¤ï¼šåŸ qwen/Qwen-0.5B ä»“åº“å·²å¤±æ•ˆï¼Œæ”¹ç”¨æœ€æ–°çš„ Qwen2.5-0.5B-Instruct
    model_id = "Qwen/Qwen2.5-0.5B-Instruct"
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    
    # 3. åŠ è½½æ¨¡å‹ (Model) - è´Ÿè´£è®¡ç®—å’Œç”Ÿæˆ
    # device_map="cpu" å¼ºåˆ¶ä½¿ç”¨ CPUï¼Œç¡®ä¿æ¶ˆè´¹çº§ç”µè„‘ä¹Ÿèƒ½è·‘
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cpu", trust_remote_code=True)
    
    # 4. å‡†å¤‡è¾“å…¥
    prompt = "æç®€ä¸»ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"
    inputs = tokenizer(prompt, return_tensors="pt")
    
    print(f"\nğŸ‘¤ ç”¨æˆ·è¾“å…¥: {prompt}")
    print("ğŸ¤– æ­£åœ¨æ€è€ƒä¸­...\n")
    
    # 5. ç”Ÿæˆå›å¤
    # max_new_tokens=100: é™åˆ¶ç”Ÿæˆé•¿åº¦ï¼Œé˜²æ­¢åºŸè¯
    # do_sample=True: è®©å›ç­”æœ‰ç‚¹éšæœºæ€§ï¼Œæ›´åƒäºº
    pred = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.7)
    
    # 6. è§£ç è¾“å‡º - æŠŠæ•°å­—å˜å›äººè¯
    response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=True)
    print(f"ğŸ’¬ æ¨¡å‹å›ç­”: {response}")

if __name__ == "__main__":
    run_inference()