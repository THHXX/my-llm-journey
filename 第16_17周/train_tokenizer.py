from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders
import json
import os

def train_custom_tokenizer():
    input_file = "processed_data.jsonl"
    if not os.path.exists(input_file):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶ {input_file}ï¼Œè¯·å…ˆè¿è¡Œ clean_data.py")
        return

    print("ğŸš‚ å¼€å§‹è®­ç»ƒè‡ªå®šä¹‰ Tokenizer (BPE)...")
    
    # 1. å‡†å¤‡è®­ç»ƒè¯­æ–™
    # Tokenizer éœ€è¦å¤§é‡æ–‡æœ¬æ¥å­¦ä¹ è¯è¡¨ã€‚æˆ‘ä»¬æå– instruction å’Œ output å­—æ®µã€‚
    corpus_file = "tokenizer_corpus.txt"
    print("ğŸ“ æ­£åœ¨ç”Ÿæˆè¯­æ–™æ–‡ä»¶...")
    with open(input_file, "r", encoding="utf-8") as f_in, \
         open(corpus_file, "w", encoding="utf-8") as f_out:
        for line in f_in:
            data = json.loads(line)
            # å°†æŒ‡ä»¤å’Œå›ç­”éƒ½ä½œä¸ºè®­ç»ƒè¯­æ–™
            f_out.write(data["instruction"] + "\n")
            f_out.write(data["output"] + "\n")
            
    print(f"ğŸ“„ è¯­æ–™å‡†å¤‡å®Œæˆ: {corpus_file}")

    # 2. åˆå§‹åŒ– Tokenizer (ä½¿ç”¨ BPE ç®—æ³•)
    # BPE (Byte-Pair Encoding) æ˜¯ç›®å‰å¤§æ¨¡å‹æœ€å¸¸ç”¨çš„åˆ†è¯ç®—æ³•
    tokenizer = Tokenizer(models.BPE())
    
    # 3. é¢„å¤„ç† (Pre-tokenization)
    # ByteLevel: å­—èŠ‚çº§å¤„ç†ï¼Œå¯¹ä»£ç å’Œå¤šè¯­è¨€æ”¯æŒæ›´å¥½ï¼ˆGPT-2/GPT-3/Llama éƒ½åœ¨ç”¨ï¼‰
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    
    # 4. é…ç½®è®­ç»ƒå™¨
    # vocab_size: è¯è¡¨å¤§å°ã€‚
    # å¯¹äºæ¼”ç¤ºé¡¹ç›®ï¼Œè®¾ä¸º 10000 è¶³å¤Ÿï¼›å•†ä¸šå¤§æ¨¡å‹é€šå¸¸æ˜¯ 32000 - 100000+
    trainer = trainers.BpeTrainer(
        vocab_size=10000, 
        min_frequency=2, # è‡³å°‘å‡ºç°2æ¬¡æ‰ä¼šè¢«æ”¶å½•
        special_tokens=["<PAD>", "<UNK>", "<BOS>", "<EOS>"]
    )
    
    # 5. å¼€å§‹è®­ç»ƒ
    print("â³ æ­£åœ¨è®­ç»ƒ Tokenizerï¼Œè¯·ç¨å€™...")
    tokenizer.train([corpus_file], trainer)
    
    # 6. åå¤„ç† (Decoding)
    # è§£ç æ—¶ä¹Ÿéœ€è¦ ByteLevel
    tokenizer.decoder = decoders.ByteLevel()
    
    # 7. ä¿å­˜
    save_path = os.path.join(base_dir, "my_custom_tokenizer.json")
    tokenizer.save(save_path)
    print(f"âœ… Tokenizer è®­ç»ƒå®Œæˆå¹¶ä¿å­˜ä¸º {save_path}")
    
    # 8. æµ‹è¯•
    print("\nğŸ§ª æ•ˆæœæµ‹è¯•:")
    test_texts = ["äººå·¥æ™ºèƒ½", "Hello World", "æ•°æ®æ¸…æ´—å¾ˆé‡è¦"]
    for text in test_texts:
        encoded = tokenizer.encode(text)
        print(f"åŸæ–‡: {text}")
        print(f"Token IDs: {encoded.ids}")
        print(f"Tokens:    {encoded.tokens}")
        print("-" * 30)

if __name__ == "__main__":
    train_custom_tokenizer()
