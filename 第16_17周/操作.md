<div style="display: flex; align-items: flex-start;">

<!-- å·¦ä¾§ç›®å½• -->
<div style="width: 200px; position: sticky; top: 0; height: 100vh; overflow-y: auto; background-color: #f6f8fa; padding: 20px; border-right: 1px solid #d0d7de; flex-shrink: 0;">

<h3 style="margin-top: 0;">ğŸ“š ç›®å½•å¯¼èˆª</h3>

1. [ä»»åŠ¡æ¦‚è§ˆ](#1-ä»»åŠ¡æ¦‚è§ˆ-overview)
2. [å‡†å¤‡å·¥ä½œ](#2-å‡†å¤‡å·¥ä½œ-setup)
3. [æ•°æ®ä¸‹è½½](#3-æ•°æ®ä¸‹è½½-download)
4. [æ•°æ®æ¸…æ´—](#4-æ•°æ®æ¸…æ´—-cleaning)
5. [Tokenizerè®­ç»ƒ](#5-tokenizer-è®­ç»ƒ-optional)
6. [åŸç†è§£æ](#6-åŸç†è§£æ-concept)
    - [æ•°æ®æµè½¬å›¾](#61-æ•°æ®æµè½¬å¯è§†åŒ–-mermaid)
    - [BPEåŸç†](#62-bpe-åˆ†è¯åŸç†)

</div>

<!-- å³ä¾§æ­£æ–‡ -->
<div style="flex-grow: 1; padding: 20px; min-width: 0;">

# ç¬¬16-17å‘¨ï¼šæ•°æ®é¢„å¤„ç†ä¸ Tokenizer å®æˆ˜

## 1. ä»»åŠ¡æ¦‚è§ˆ (Overview)

åœ¨å¤§æ¨¡å‹è®­ç»ƒæµç¨‹ä¸­ï¼Œ**æ•°æ®è´¨é‡å†³å®šäº†æ¨¡å‹çš„ä¸Šé™**ã€‚æœ¬å‘¨æˆ‘ä»¬æ¨¡æ‹ŸçœŸå®çš„æ•°æ®å‡†å¤‡æµç¨‹ã€‚

*   **ç›®æ ‡**ï¼šæ„å»ºä¸€ä»½é«˜è´¨é‡çš„æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰æ•°æ®é›†ã€‚
*   **è¾“å…¥**ï¼šå¼€æºæ•°æ®é›† `alpaca_chinese` (åŸå§‹è„æ•°æ®)ã€‚
*   **è¾“å‡º**ï¼š`processed_data.jsonl` (æ¸…æ´—åçš„é»„é‡‘æ•°æ®) + è‡ªå®šä¹‰ `Tokenizer`ã€‚

---

## 2. å‡†å¤‡å·¥ä½œ (Setup)

æˆ‘ä»¬éœ€è¦å®‰è£…å¤„ç†æ•°æ®å¸¸ç”¨çš„åº“ã€‚

```bash
# datasets: Hugging Face å®˜æ–¹æ•°æ®åŠ è½½åº“
# pandas: å¼ºå¤§çš„æ•°æ®åˆ†æåº“ï¼Œæ¸…æ´—ç¥å™¨
# tokenizers: è®­ç»ƒåˆ†è¯å™¨çš„åº“ (Rustå†…æ ¸ï¼Œè¶…å¿«)
pip install datasets pandas tokenizers
```

---

## 3. æ•°æ®ä¸‹è½½ (Download)

å›½å†…ç½‘ç»œç›´æ¥è®¿é—® Hugging Face å¯èƒ½ä¼šå¤±è´¥ï¼Œæˆ‘ä»¬éœ€è¦é…ç½®é•œåƒå¹¶ç¼–å†™ä¸‹è½½è„šæœ¬ã€‚

### ğŸ“„ ä»£ç å®ç°ï¼š`download_data.py`

```python
import os
from datasets import load_dataset

# ğŸš€ å…³é”®ï¼šè®¾ç½® HF é•œåƒï¼Œè§£å†³ç½‘ç»œé—®é¢˜
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

def download_alpaca_data():
    print("ğŸš€ å¼€å§‹ä¸‹è½½ alpaca_chinese æ•°æ®é›†...")
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset("qingxu98/alpaca_chinese", split="train")
    
    # ä¿å­˜åŸå§‹æ•°æ®
    dataset.to_json("raw_data.json", force_ascii=False, indent=4)
    print(f"âœ… ä¸‹è½½å®Œæˆï¼å·²ä¿å­˜ raw_data.json")

if __name__ == "__main__":
    download_alpaca_data()
```

### â–¶ï¸ è¿è¡Œ
```bash
python download_data.py
```

---

## 4. æ•°æ®æ¸…æ´— (Cleaning)

åŸå§‹æ•°æ®å¯èƒ½åŒ…å«ï¼š
1.  **é‡å¤æ•°æ®**ï¼šæµªè´¹è®­ç»ƒæ—¶é—´ã€‚
2.  **å¼‚å¸¸æ•°æ®**ï¼šä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ã€‚
3.  **æ ¼å¼è½¬æ¢**ï¼šè®­ç»ƒæ¡†æ¶é€šå¸¸éœ€è¦ `jsonl` æ ¼å¼ã€‚

### ğŸ“„ ä»£ç å®ç°ï¼š`clean_data.py`

```python
import pandas as pd

def clean_data():
    # 1. è¯»å–
    df = pd.read_json("raw_data.json")
    print(f"åŸå§‹æ•°é‡: {len(df)}")
    
    # 2. æ¸…æ´—
    # å»é‡
    df = df.drop_duplicates(subset=['instruction', 'input'])
    # å»ç©º
    df = df.dropna(subset=['output'])
    # è¿‡æ»¤è¿‡çŸ­å›ç­”
    df = df[df['output'].str.len() > 1]
    
    print(f"æ¸…æ´—åæ•°é‡: {len(df)}")
    
    # 3. ä¿å­˜ä¸º JSONL
    df.to_json("processed_data.jsonl", orient='records', lines=True, force_ascii=False)
    print("âœ… å·²ä¿å­˜ processed_data.jsonl")

if __name__ == "__main__":
    clean_data()
```

### â–¶ï¸ è¿è¡Œä¸éªŒæ”¶
```bash
python clean_data.py
```
**éªŒæ”¶æ ‡å‡†**ï¼šæ£€æŸ¥å½“å‰ç›®å½•ä¸‹ç”Ÿæˆçš„ `processed_data.jsonl` æ–‡ä»¶ï¼Œç¡®ä¿åŒ…å« 1000+ æ¡æ•°æ®ï¼Œä¸”æ¯è¡Œéƒ½æ˜¯ä¸€ä¸ªå®Œæ•´çš„ JSONã€‚

---

## 5. Tokenizer è®­ç»ƒ (Optional)

**ä¸ºä»€ä¹ˆéœ€è¦è‡ªå®šä¹‰ Tokenizerï¼Ÿ**
é€šç”¨ Tokenizerï¼ˆå¦‚ GPT-4 çš„ï¼‰åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚å¤æ–‡ã€åŒ»å­¦ä»£ç ï¼‰å¯èƒ½æ•ˆç‡ä½ä¸‹ã€‚è®­ç»ƒè‡ªå·±çš„ Tokenizer å¯ä»¥è®©æ¨¡å‹â€œè¯»â€å¾—æ›´å‡†ã€‚

### ğŸ“„ ä»£ç å®ç°ï¼š`train_tokenizer.py`

```python
from tokenizers import Tokenizer, models, trainers, pre_tokenizers

# 1. åˆå§‹åŒ– BPE æ¨¡å‹
tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)

# 2. é…ç½®è®­ç»ƒå™¨
trainer = trainers.BpeTrainer(
    vocab_size=10000,  # è¯è¡¨å¤§å°
    special_tokens=["<PAD>", "<UNK>", "<BOS>", "<EOS>"]
)

# 3. è®­ç»ƒ (ä½¿ç”¨æˆ‘ä»¬æ¸…æ´—å¥½çš„æ•°æ®)
# æå–æ–‡æœ¬ç”Ÿæˆä¸´æ—¶è¯­æ–™æ–‡ä»¶ç•¥... (è§å®Œæ•´è„šæœ¬)
tokenizer.train(["tokenizer_corpus.txt"], trainer)

# 4. ä¿å­˜
tokenizer.save("my_custom_tokenizer.json")
```

### â–¶ï¸ è¿è¡Œ
```bash
python train_tokenizer.py
```

---

## 6. åŸç†è§£æ (Concept)

### 6.1 æ•°æ®æµè½¬å¯è§†åŒ– (Mermaid)

```mermaid
graph LR
    A[Hugging Face Cloud] -->|download_data.py| B(raw_data.json)
    B -->|clean_data.py| C{Pandas æ¸…æ´—}
    C -->|å»é‡/å»ç©º| D(processed_data.jsonl)
    D -->|è¾“å…¥| E[LLM æ¨¡å‹è®­ç»ƒ]
    D -->|æå–æ–‡æœ¬| F[Tokenizer è®­ç»ƒ]
    F -->|ç”Ÿæˆ| G(vocab.json)
    
    style A fill:#e1f5fe
    style B fill:#fff9c4
    style D fill:#c8e6c9
    style G fill:#ffccbc
```

### 6.2 BPE åˆ†è¯åŸç†
**BPE (Byte-Pair Encoding)** æ˜¯ä¸€ç§ç»Ÿè®¡åˆ†è¯æ–¹æ³•ã€‚
1.  **åˆå§‹çŠ¶æ€**ï¼šæŠŠæ‰€æœ‰è¯æ‹†æˆå­—æ¯ã€‚`h u g`, `p u g`
2.  **ç»Ÿè®¡é¢‘ç‡**ï¼šå‘ç° `u` å’Œ `g` ç»å¸¸ä¸€èµ·å‡ºç°ã€‚
3.  **åˆå¹¶**ï¼šåˆ›é€ æ–° token `ug`ã€‚
4.  **è¿­ä»£**ï¼šç°åœ¨æœ‰ `h ug`, `p ug`ã€‚ç»§ç»­ç»Ÿè®¡...
5.  **ç»“æœ**ï¼šé«˜é¢‘è¯ï¼ˆå¦‚ `ing`, `tion`ï¼‰è¢«åˆå¹¶æˆä¸€ä¸ª tokenï¼Œç”Ÿåƒ»è¯ä¿ç•™ä¸ºå­—æ¯åºåˆ—ã€‚

**å¥½å¤„**ï¼š
*   **è¯è¡¨å¯æ§**ï¼šä¸ä¼šæ— é™è†¨èƒ€ã€‚
*   **æ—  OOV é—®é¢˜**ï¼šä»»ä½•ç”Ÿè¯éƒ½èƒ½ç”±åŸºç¡€å­—ç¬¦æ‹¼å‡ºæ¥ã€‚

</div>
</div>
