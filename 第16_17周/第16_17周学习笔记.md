<div style="display: flex; align-items: flex-start;">

<!-- å·¦ä¾§ç›®å½• -->
<div style="width: 200px; position: sticky; top: 0; height: 100vh; overflow-y: auto; background-color: #f6f8fa; padding: 20px; border-right: 1px solid #d0d7de; flex-shrink: 0;">

<h3 style="margin-top: 0;">ğŸ“š ç›®å½•å¯¼èˆª</h3>

1. [æ ¸å¿ƒä»»åŠ¡](#1-æ ¸å¿ƒä»»åŠ¡)
2. [æŠ€æœ¯æœ¯è¯­è¡¨](#2-æŠ€æœ¯æœ¯è¯­è¡¨)
3. [æ–‡ä»¶æ¸…å•ä¸ä½œç”¨](#3-æ–‡ä»¶æ¸…å•ä¸ä½œç”¨)
4. [æµç¨‹å¯è§†åŒ–](#4-æµç¨‹å¯è§†åŒ–)
5. [è¯¦ç»†æ“ä½œæ­¥éª¤](#5-è¯¦ç»†æ“ä½œæ­¥éª¤)
   - [ä¸‹è½½æ•°æ®](#51-ä¸‹è½½æ•°æ®)
   - [æ•°æ®æ¸…æ´—](#52-æ•°æ®æ¸…æ´—)
   - [è®­ç»ƒ Tokenizer](#53-è®­ç»ƒ-tokenizer)
6. [é‡åˆ°çš„å‘ä¸è§£å†³](#6-é‡åˆ°çš„å‘ä¸è§£å†³)

</div>

<!-- å³ä¾§æ­£æ–‡ -->
<div style="flex-grow: 1; padding: 20px; min-width: 0;">

# ç¬¬16-17å‘¨å­¦ä¹ ç¬”è®°ï¼šæ•°æ®é¢„å¤„ç†ä¸ Tokenizer è®­ç»ƒ

## 1. æ ¸å¿ƒä»»åŠ¡

æœ¬å‘¨ä¸»è¦å®Œæˆäº†å¤§æ¨¡å‹è®­ç»ƒå‰çš„å‡†å¤‡å·¥ä½œï¼š**æ•°æ®é¢„å¤„ç†**å’Œ**åˆ†è¯å™¨ï¼ˆTokenizerï¼‰è®­ç»ƒ**ã€‚
è¿™æ˜¯ä»â€œä½¿ç”¨æ¨¡å‹â€è¿›é˜¶åˆ°â€œè®­ç»ƒæ¨¡å‹â€çš„å…³é”®ä¸€æ­¥ã€‚æ¨¡å‹ä¸æ‡‚ä¸­æ–‡ï¼Œä¹Ÿä¸æ‡‚è‹±æ–‡ï¼Œå®ƒåªè®¤è¯†æ•°å­—ã€‚Tokenizer çš„ä½œç”¨å°±æ˜¯æŠŠäººç±»è¯­è¨€é«˜æ•ˆåœ°è½¬æ¢æˆæ¨¡å‹èƒ½è¯»æ‡‚çš„æ•°å­—åºåˆ—ã€‚

## 2. æŠ€æœ¯æœ¯è¯­è¡¨

| è‹±æ–‡ Term | ä¸­æ–‡ | æç®€è§£é‡Š | 
| :--- | :--- | :--- |
| **Tokenizer** | åˆ†è¯å™¨ | ç¿»è¯‘å®˜ã€‚æŠŠâ€œæˆ‘çˆ±AIâ€åˆ‡åˆ†æˆâ€œæˆ‘â€ã€â€œçˆ±â€ã€â€œAIâ€ï¼Œç„¶åè½¬æˆæ•°å­— [101, 203, 305]ã€‚ |
| **Corpus** | è¯­æ–™åº“ | æ¨¡å‹çš„è¯¾æœ¬ã€‚ä¸€å¤§å †ç”¨æ¥è®­ç»ƒçš„æ–‡æœ¬æ•°æ®ã€‚ |
| **BPE (Byte-Pair Encoding)** | å­—èŠ‚å¯¹ç¼–ç  | ä¸€ç§æ™ºèƒ½åˆ†è¯ç®—æ³•ã€‚å®ƒä¸æŒ‰å•è¯åˆ‡ï¼Œè€Œæ˜¯æŠŠå¸¸è§çš„å­—ç»„åˆåœ¨ä¸€èµ·ï¼ˆå¦‚â€œäººå·¥â€+â€œæ™ºèƒ½â€ï¼‰ï¼Œç”Ÿåƒ»å­—æ‹†å¼€ã€‚æ•ˆç‡é«˜ï¼Œè¯è¡¨å°ã€‚ |
| **JSONL** | JSON Lines | æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡çš„æ–‡ä»¶æ ¼å¼ã€‚æ¯”æ™®é€š JSON æ›´é€‚åˆå¤„ç†æµ·é‡æ•°æ®ï¼Œå› ä¸ºå¯ä»¥è¯»ä¸€è¡Œå¤„ç†ä¸€è¡Œï¼Œä¸å å†…å­˜ã€‚ |
| **Vocab Size** | è¯è¡¨å¤§å° | ç¿»è¯‘å®˜è®¤è¯†çš„å•è¯æ€»é‡ã€‚æœ¬æ¬¡è®¾ç½®ä¸º 10,000ã€‚ |

## 3. æ–‡ä»¶æ¸…å•ä¸ä½œç”¨

æ‰€æœ‰æ–‡ä»¶å‡å·²å½’æ¡£è‡³ `ç¬¬16_17å‘¨` æ–‡ä»¶å¤¹ä¸‹ã€‚

| æ–‡ä»¶å | ç±»å‹ | ä½œç”¨ |
| :--- | :--- | :--- |
| `raw_data.json` | **åŸå§‹æ•°æ®** | ä» Hugging Face ä¸‹è½½ä¸‹æ¥çš„åˆå§‹æ•°æ®ã€‚åŒ…å«æŒ‡ä»¤ã€è¾“å…¥å’Œè¾“å‡ºï¼Œä½†å¯èƒ½å­˜åœ¨é‡å¤ã€ç©ºå€¼æˆ–æ ¼å¼é—®é¢˜ã€‚ |
| `processed_data.jsonl` | **æ¸…æ´—åæ•°æ®** | ç»è¿‡ä»£ç å»é‡ã€å»ç©ºã€æ ¼å¼åŒ–åçš„â€œå¹²å‡€â€æ•°æ®ã€‚æ¯è¡Œä¸€æ¡ï¼Œé€‚åˆæ¨¡å‹è¯»å–ã€‚ |
| `tokenizer_corpus.txt` | **è®­ç»ƒè¯­æ–™** | ä¸“é—¨æå–å‡ºæ¥çš„çº¯æ–‡æœ¬æ–‡ä»¶ï¼ˆåªåŒ…å«æ–‡å­—ï¼‰ã€‚ç”¨æ¥å–‚ç»™ Tokenizer å­¦ä¹ å¦‚ä½•åˆ†è¯ã€‚ |
| `my_custom_tokenizer.json` | **æ¨¡å‹æ–‡ä»¶** | **æœ€ç»ˆäº§ç‰©**ã€‚è®­ç»ƒå¥½çš„ Tokenizer æ¨¡å‹ï¼Œé‡Œé¢è®°å½•äº†å®ƒå­¦ä¼šçš„ 10,000 ä¸ªè¯æ±‡å’Œç¼–ç è§„åˆ™ã€‚ |
| `download_data.py` | è„šæœ¬ | è´Ÿè´£ä»ç½‘ç»œä¸‹è½½æ•°æ®å¹¶ä¿å­˜ã€‚ |
| `clean_data.py` | è„šæœ¬ | è´Ÿè´£æŠŠè„æ•°æ®å˜æˆå¹²å‡€æ•°æ®ã€‚ |
| `train_tokenizer.py` | è„šæœ¬ | è´Ÿè´£è¯»å–æ–‡æœ¬ï¼Œè®­ç»ƒå‡º Tokenizer æ¨¡å‹ã€‚ |

## 4. æµç¨‹å¯è§†åŒ–

```mermaid
graph LR
    A[Hugging Face äº‘ç«¯] -->|download_data.py| B(raw_data.json<br>åŸå§‹æ•°æ®)
    B -->|clean_data.py| C(processed_data.jsonl<br>æ¸…æ´—æ•°æ®)
    C -->|æå–æ–‡æœ¬| D(tokenizer_corpus.txt<br>çº¯æ–‡æœ¬è¯­æ–™)
    D -->|train_tokenizer.py| E{BPE ç®—æ³•è®­ç»ƒ}
    E --> F[my_custom_tokenizer.json<br>åˆ†è¯å™¨æ¨¡å‹]
    
    style B fill:#f9f,stroke:#333
    style C fill:#bbf,stroke:#333
    style F fill:#bfb,stroke:#333
```

## 5. è¯¦ç»†æ“ä½œæ­¥éª¤

### 5.1 ä¸‹è½½æ•°æ®
ç”±äº Hugging Face å›½å†…è®¿é—®ä¸ç¨³å®šï¼Œæˆ‘ä»¬é‡‡å–äº†çµæ´»ç­–ç•¥ã€‚å¦‚æœè®¾ç½®é•œåƒæ— æ•ˆï¼Œç›´æ¥å¼€å¯ç§‘å­¦ä¸Šç½‘å·¥å…·å³å¯ã€‚

**ä»£ç  (`download_data.py`):**
```python
import os
from datasets import load_dataset
import json

def download_alpaca_data():
    print("ğŸš€ å¼€å§‹ä¸‹è½½ alpaca_chinese æ•°æ®é›†...")
    
    # æ›¿æ¢ä¸ºæ›´ç¨³å®šçš„æ•°æ®é›†æºï¼šsilk-road/alpaca-data-gpt4-chinese
    dataset = load_dataset("silk-road/alpaca-data-gpt4-chinese", split="train")
    
    # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿æ–‡ä»¶ç”Ÿæˆåœ¨è„šæœ¬åŒçº§ç›®å½•
    output_file = os.path.join(os.path.dirname(__file__), "raw_data.json")
    
    # ä¿å­˜ä¸ºæ ‡å‡† JSON åˆ—è¡¨æ ¼å¼
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(list(dataset), f, ensure_ascii=False, indent=4)
        
    print(f"ğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    download_alpaca_data()
```

### 5.2 æ•°æ®æ¸…æ´—
æ¸…æ´—æ˜¯æ•°æ®ç§‘å­¦ä¸­æœ€è€—æ—¶ä½†æœ€é‡è¦çš„æ­¥éª¤ã€‚åƒåœ¾è¿›ï¼Œåƒåœ¾å‡º (Garbage In, Garbage Out)ã€‚

**æ¸…æ´—é€»è¾‘ï¼š**
1.  **è¯»å–**ï¼šå…¼å®¹æ ‡å‡† JSON å’Œ JSONL æ ¼å¼ã€‚
2.  **å»é‡**ï¼šåˆ é™¤å®Œå…¨ä¸€æ ·çš„æŒ‡ä»¤ã€‚
3.  **å»ç©º**ï¼šåˆ é™¤ç¼ºå°‘å›ç­”çš„æ•°æ®ã€‚
4.  **ä¿å­˜**ï¼šè½¬å­˜ä¸º `JSONL` æ ¼å¼ï¼Œæ–¹ä¾¿æµå¼è¯»å–ã€‚

**ä»£ç  (`clean_data.py`):**
```python
import json
import pandas as pd
import os

def clean_data():
    base_dir = os.path.dirname(__file__)
    input_file = os.path.join(base_dir, "raw_data.json")
    output_file = os.path.join(base_dir, "processed_data.jsonl")
    
    # è¯»å–æ•°æ®
    try:
        df = pd.read_json(input_file)
    except ValueError:
        df = pd.read_json(input_file, lines=True)
    
    # å»é‡ä¸å»ç©º
    df_clean = df.drop_duplicates(subset=['instruction', 'input'])
    df_clean = df_clean.dropna(subset=['instruction', 'output'])
    df_clean = df_clean[df_clean['output'].str.len() > 1]
    
    # ä¿å­˜ä¸º JSONL
    df_clean.to_json(output_file, orient='records', lines=True, force_ascii=False)
    print(f"âœ… æ¸…æ´—å®Œæˆï¼å·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    clean_data()
```

### 5.3 è®­ç»ƒ Tokenizer
ä½¿ç”¨ Hugging Face çš„ `tokenizers` åº“è®­ç»ƒä¸€ä¸ª BPE åˆ†è¯å™¨ã€‚

**å…³é”®å‚æ•°ï¼š**
*   `vocab_size=10000`: è¯è¡¨å¤§å°ã€‚
*   `min_frequency=2`: å‡ºç°å°‘äº2æ¬¡çš„è¯ä¼šè¢«å¿½ç•¥ï¼ˆé™å™ªï¼‰ã€‚
*   `special_tokens`: ç‰¹æ®Šæ ‡è®°ï¼Œå¦‚ `<PAD>` (è¡¥å…¨), `<EOS>` (å¥å°¾)ã€‚

**ä»£ç  (`train_tokenizer.py`):**
```python
from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders
import json
import os

def train_custom_tokenizer():
    base_dir = os.path.dirname(__file__)
    input_file = os.path.join(base_dir, "processed_data.jsonl")
    corpus_file = os.path.join(base_dir, "tokenizer_corpus.txt")
    
    # 1. æå–è¯­æ–™
    with open(input_file, "r", encoding="utf-8") as f_in, \
         open(corpus_file, "w", encoding="utf-8") as f_out:
        for line in f_in:
            data = json.loads(line)
            f_out.write(data["instruction"] + "\n")
            f_out.write(data["output"] + "\n")
            
    # 2. è®­ç»ƒ Tokenizer
    tokenizer = Tokenizer(models.BPE())
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    
    trainer = trainers.BpeTrainer(
        vocab_size=10000, 
        min_frequency=2,
        special_tokens=["<PAD>", "<UNK>", "<BOS>", "<EOS>"]
    )
    
    tokenizer.train([corpus_file], trainer)
    tokenizer.save(os.path.join(base_dir, "my_custom_tokenizer.json"))
    print("âœ… Tokenizer è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    train_custom_tokenizer()
```

## 6. é‡åˆ°çš„å‘ä¸è§£å†³

1.  **HF é•œåƒæ— æ•ˆ**ï¼š
    *   **ç°è±¡**ï¼šè®¾ç½®äº† `HF_ENDPOINT` ä»ç„¶ä¸‹è½½å¤±è´¥æˆ–è¶…æ—¶ã€‚
    *   **è§£å†³**ï¼šæœ€ç›´æ¥çš„åŠæ³•æ˜¯**å¼€å¯ç§‘å­¦ä¸Šç½‘**ï¼Œå¹¶æ³¨é‡Šæ‰ä»£ç ä¸­çš„é•œåƒè®¾ç½®ã€‚

2.  **æ•°æ®æ ¼å¼æ··ä¹±**ï¼š
    *   **ç°è±¡**ï¼š`dataset.to_json(indent=4)` ä¼šç”Ÿæˆéæ³•çš„ JSONL æ ¼å¼ï¼ˆå¸¦ç¼©è¿›ä½†æ²¡é€—å·ï¼‰ï¼Œå¯¼è‡´ Pandas è¯»å–æŠ¥é”™ã€‚
    *   **è§£å†³**ï¼šæ”¾å¼ƒ `to_json`ï¼Œä½¿ç”¨ `json.dump(list(dataset))` æ‰‹åŠ¨ä¿å­˜ä¸ºæ ‡å‡† JSON æ•°ç»„æ ¼å¼ã€‚

3.  **è·¯å¾„é—®é¢˜**ï¼š
    *   **ç°è±¡**ï¼šè„šæœ¬åœ¨ä¸åŒç›®å½•ä¸‹è¿è¡Œï¼Œç”Ÿæˆçš„æ–‡ä»¶ä½ç½®ä¹±è·‘ã€‚
    *   **è§£å†³**ï¼šä½¿ç”¨ `os.path.join(os.path.dirname(__file__), "æ–‡ä»¶å")` é”å®šæ–‡ä»¶è·¯å¾„ä¸ºè„šæœ¬æ‰€åœ¨ç›®å½•ã€‚

</div>
</div>
