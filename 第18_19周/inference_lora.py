import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import os

def inference():
    # è·¯å¾„é…ç½®
    base_model_path = "qwen/Qwen1.5-0.5B"
    lora_path = os.path.join(os.path.dirname(__file__), "lora_output")
    
    print(f"ğŸš€ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
    base_model = AutoModelForCausalLM.from_pretrained(base_model_path, device_map="auto", trust_remote_code=True)
    
    print(f"ğŸ”— åŠ è½½ LoRA æƒé‡: {lora_path}")
    # åŠ è½½å¾®è°ƒåçš„æƒé‡
    try:
        model = PeftModel.from_pretrained(base_model, lora_path)
    except Exception as e:
        print(f"âŒ åŠ è½½ LoRA å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ train_lora.py å®Œæˆè®­ç»ƒ")
        return

    model.eval()
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "ä»€ä¹ˆæ˜¯åˆåŒæ³•ï¼Ÿ",
        "ç”²æ–¹è¿çº¦äº†æ€ä¹ˆåŠï¼Ÿ",
        "åŒ»ç”Ÿæœ‰å‘ŠçŸ¥ä¹‰åŠ¡å—ï¼Ÿ", # è®­ç»ƒé›†é‡Œçš„é—®é¢˜
        "è§£é‡Šä¸€ä¸‹ä¸å¯æŠ—åŠ›ã€‚" # ç¨å¾®å˜ä½“
    ]
    
    print("\nğŸ’¬ å¼€å§‹å¯¹è¯æµ‹è¯•:")
    print("="*50)
    
    for q in test_questions:
        prompt = f"Instruction: {q}\nInput: \nOutput: "
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=100, 
                do_sample=False # è®¾ä¸º False æ–¹ä¾¿å¤ç°
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # æå– Output åé¢çš„å†…å®¹
        try:
            answer = response.split("Output: ")[1].strip()
        except:
            answer = response
            
        print(f"â“ é—®: {q}")
        print(f"ğŸ¤– ç­”: {answer}")
        print("-" * 30)

if __name__ == "__main__":
    inference()
