import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType

# 1. é…ç½®å‚æ•°
model_id = "qwen/Qwen1.5-0.5B"  # åŸºç¡€æ¨¡å‹
data_path = os.path.join(os.path.dirname(__file__), "law_data.json") # è®­ç»ƒæ•°æ®
output_dir = os.path.join(os.path.dirname(__file__), "lora_output") # è¾“å‡ºç›®å½•

# 2. åŠ è½½ Tokenizer
print(f"ğŸš€ æ­£åœ¨åŠ è½½ Tokenizer: {model_id}...")
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token # Qwen çš„ pad token è®¾ç½®

# 3. åŠ è½½æ•°æ®é›†
print(f"ğŸ“š æ­£åœ¨åŠ è½½æ•°æ®é›†: {data_path}...")
dataset = load_dataset("json", data_files=data_path, split="train")

# æ•°æ®é¢„å¤„ç†å‡½æ•°
def process_func(example):
    MAX_LENGTH = 256
    input_ids, attention_mask, labels = [], [], []
    
    # æ„å»º Prompt: 
    # Instruction: ...
    # Input: ...
    # Output: ...
    instruction = example["instruction"]
    inputs = example.get("input", "")
    response = example["output"]
    
    prompt = f"Instruction: {instruction}\nInput: {inputs}\nOutput: "
    
    # ç¼–ç 
    instruction_ids = tokenizer.encode(prompt, add_special_tokens=True)
    response_ids = tokenizer.encode(response, add_special_tokens=False) + [tokenizer.eos_token_id]
    
    # æ‹¼æ¥
    input_ids = instruction_ids + response_ids
    attention_mask = [1] * len(input_ids)
    
    # Labels (Instruction éƒ¨åˆ†è®¾ä¸º -100ï¼Œä¸è®¡ç®— Loss)
    labels = [-100] * len(instruction_ids) + response_ids
    
    # æˆªæ–­
    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

tokenized_id = dataset.map(process_func, remove_columns=dataset.column_names)

# 4. åŠ è½½æ¨¡å‹
print(f"ğŸ§  æ­£åœ¨åŠ è½½æ¨¡å‹: {model_id}...")
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡ (GPU/CPU)
    trust_remote_code=True
)

# å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼Œå¿…é¡»æ˜¾å¼å¼€å¯è¾“å…¥çš„æ¢¯åº¦è®¡ç®—
model.enable_input_require_grads()

# 5. é…ç½® LoRA
print("ğŸ”§ é…ç½® LoRA...")
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, 
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False, # è®­ç»ƒæ¨¡å¼
    r=8, # LoRA ç§©ï¼Œè¶Šå¤§å‚æ•°è¶Šå¤š
    lora_alpha=32, # LoRA ç¼©æ”¾ç³»æ•°
    lora_dropout=0.1 # Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
)

model = get_peft_model(model, config)
model.print_trainable_parameters() # æ‰“å°å¯è®­ç»ƒå‚æ•°é‡

# 6. é…ç½®è®­ç»ƒå‚æ•°
args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=4, # æ‰¹æ¬¡å¤§å°ï¼Œæ˜¾å­˜ä¸å¤Ÿæ”¹å°
    gradient_accumulation_steps=4, # æ¢¯åº¦ç´¯ç§¯
    logging_steps=10,
    num_train_epochs=3, # è®­ç»ƒè½®æ•°
    save_steps=50, 
    learning_rate=1e-4,
    save_on_each_node=True,
    gradient_checkpointing=True, # èŠ‚çœæ˜¾å­˜
)

# 7. å¼€å§‹è®­ç»ƒ
print("ğŸ‹ï¸â€â™‚ï¸ å¼€å§‹è®­ç»ƒ...")
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_id,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
)

trainer.train()

# 8. ä¿å­˜æ¨¡å‹
print(f"ğŸ’¾ ä¿å­˜ LoRA æƒé‡åˆ° {output_dir}")
trainer.save_model(output_dir)
print("âœ… è®­ç»ƒå®Œæˆï¼")
