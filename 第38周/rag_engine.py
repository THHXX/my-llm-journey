import os
import shutil
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.chat_models import ChatTongyi
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

class SmartCSBot:
    def __init__(self, data_dir="./data", model_name="qwen-max"):
        self.data_dir = data_dir
        self.model_name = model_name
        self.vectorstore = None
        self.qa_chain = None
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            input_key="question", 
            output_key="answer"
        )
        
        # å¼ºåˆ¶ç¦ç”¨ä»£ç†
        os.environ.pop('http_proxy', None)
        os.environ.pop('https_proxy', None)
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        self.init_vectorstore()
        
    def init_vectorstore(self):
        """åˆå§‹åŒ–æˆ–åŠ è½½å‘é‡æ•°æ®åº“"""
        print("ğŸ“¦ æ­£åœ¨åŠ è½½çŸ¥è¯†åº“...")
        
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
            print(f"âš ï¸ è­¦å‘Š: æ•°æ®ç›®å½• {self.data_dir} ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºã€‚è¯·æ”¾å…¥ txt æ–‡ä»¶ã€‚")
            return

        # åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰ txt æ–‡ä»¶
        loader = DirectoryLoader(self.data_dir, glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
        docs = loader.load()
        
        if not docs:
            print("âš ï¸ è­¦å‘Š: çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·åœ¨ data ç›®å½•ä¸‹æ”¾å…¥ txt æ–‡æ¡£ã€‚")
            return

        # åˆ‡åˆ†æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        texts = text_splitter.split_documents(docs)
        
        # åˆ›å»ºå‘é‡åº“
        embeddings = DashScopeEmbeddings(model="text-embedding-v1")
        self.vectorstore = FAISS.from_documents(texts, embeddings)
        
        # åˆ›å»º QA é“¾
        self.init_qa_chain()
        print("âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼")

    def init_qa_chain(self):
        """åˆå§‹åŒ–é—®ç­”é“¾"""
        if not self.vectorstore:
            return
            
        llm = ChatTongyi(model=self.model_name)
        
        # è‡ªå®šä¹‰ Promptï¼Œèµ‹äºˆè§’è‰²
        # æ³¨æ„ï¼šä½¿ç”¨ ConversationalRetrievalChain æ—¶ï¼Œchat_history ä¸»è¦ç”¨äºç”Ÿæˆç‹¬ç«‹é—®é¢˜
        # è¿™é‡Œæ˜¯å›ç­”é—®é¢˜çš„ Promptï¼Œåªéœ€è¦ context å’Œ question
        prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†æ™ºèƒ½å®¢æœâ€œå°èœœâ€ã€‚
è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·ç¤¼è²Œåœ°å›ç­”â€œæŠ±æ­‰ï¼Œæš‚æ—¶æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è”ç³»äººå·¥å®¢æœâ€ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
å›ç­”è¦äº²åˆ‡ã€è‡ªç„¶ï¼Œå¯ä»¥ä½¿ç”¨ emojiã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

ç”¨æˆ·æé—®ï¼š{question}
å®¢æœå›ç­”ï¼š"""

        PROMPT = PromptTemplate(
            template=prompt_template, 
            input_variables=["context", "question"]
        )
        
        # ä½¿ç”¨ ConversationalRetrievalChainï¼Œå®ƒæ˜¯å¤„ç†å¤šè½®å¯¹è¯ RAG çš„æ ‡å‡†æ–¹å¼
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
            memory=self.memory,
            combine_docs_chain_kwargs={"prompt": PROMPT}
        )

    def chat(self, query):
        """å¯¹å¤–æä¾›çš„å¯¹è¯æ¥å£"""
        if not self.qa_chain:
            return "âš ï¸ ç³»ç»Ÿæœªå°±ç»ªï¼šçŸ¥è¯†åº“ä¸ºç©ºæˆ–åˆå§‹åŒ–å¤±è´¥ã€‚"
            
        try:
            # ä½¿ç”¨ invoke è€Œä¸æ˜¯ runï¼Œå…¼å®¹æ–°ç‰ˆ LangChain
            # ConversationalRetrievalChain æ¥å— question
            result = self.qa_chain.invoke({"question": query})
            return result['answer']
        except Exception as e:
            return f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"

    def clear_memory(self):
        self.memory.clear()
