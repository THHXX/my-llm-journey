<div style="display: flex; align-items: flex-start;">

<!-- å·¦ä¾§ç›®å½• -->
<div style="width: 200px; position: sticky; top: 0; height: 100vh; overflow-y: auto; background-color: #f6f8fa; padding: 20px; border-right: 1px solid #d0d7de; flex-shrink: 0;">

<h3 style="margin-top: 0;">ğŸ“š ç›®å½•å¯¼èˆª</h3>

1. [é¡¹ç›®æ¦‚è§ˆ](#1-é¡¹ç›®æ¦‚è§ˆ)
2. [æŠ€æœ¯æ¶æ„](#2-æŠ€æœ¯æ¶æ„)
3. [è¯¦ç»†æ“ä½œæ­¥éª¤](#3-è¯¦ç»†æ“ä½œæ­¥éª¤)
   - [ç¯å¢ƒå®‰è£…](#31-ç¯å¢ƒå®‰è£…)
   - [çŸ¥è¯†åº“å‡†å¤‡](#32-çŸ¥è¯†åº“å‡†å¤‡)
   - [ç¼–å†™ RAG å¼•æ“](#33-ç¼–å†™-rag-å¼•æ“)
   - [ç¼–å†™ Gradio ç•Œé¢](#34-ç¼–å†™-gradio-ç•Œé¢)
   - [è¿è¡Œä¸æµ‹è¯•](#35-è¿è¡Œä¸æµ‹è¯•)
4. [éªŒè¯ç›®æ ‡](#4-éªŒè¯ç›®æ ‡)

</div>

<!-- å³ä¾§æ­£æ–‡ -->
<div style="flex-grow: 1; padding: 20px; min-width: 0;">

# ç¬¬22-24å‘¨ï¼šæ³•å¾‹é—®ç­”æœºå™¨äºº (RAG + å¾®è°ƒ + Gradio)

## 1. é¡¹ç›®æ¦‚è§ˆ

è¿™æ˜¯æœ¬è¯¾ç¨‹çš„**æœ€ç»ˆå¤§ä½œä¸š**ã€‚æˆ‘ä»¬å°†ç»“åˆä¹‹å‰å­¦åˆ°çš„æ‰€æœ‰æŠ€èƒ½ï¼Œæ‰“é€ ä¸€ä¸ªçœŸæ­£çš„ AI åº”ç”¨ã€‚

*   **æ ¸å¿ƒç—›ç‚¹**ï¼šå¤§æ¨¡å‹è™½ç„¶æ‡‚å¾ˆå¤šï¼Œä½†æ³•å¾‹æ¡æ¬¾æ›´æ–°å¿«ã€ç»†èŠ‚å¤šï¼Œæ¨¡å‹å®¹æ˜“â€œäº§ç”Ÿå¹»è§‰â€ï¼ˆèƒ¡è¯´å…«é“ï¼‰ã€‚
*   **è§£å†³æ–¹æ¡ˆ**ï¼š**RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)**ã€‚
    *   åœ¨å›ç­”ä¹‹å‰ï¼Œå…ˆå»â€œæ³•æ¡åº“â€é‡ŒæŸ¥é˜…ç›¸å…³èµ„æ–™ã€‚
    *   æŠŠæŸ¥åˆ°çš„èµ„æ–™æ‰”ç»™æ¨¡å‹ï¼šâ€œè¯·æ ¹æ®è¿™äº›èµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜â€ã€‚
    *   è¿™æ ·å›ç­”æ—¢ä¸“ä¸šåˆæœ‰æ®å¯ä¾ã€‚

## 2. æŠ€æœ¯æ¶æ„

```mermaid
graph TD
    User[ç”¨æˆ·] -->|æé—®: ç¦»å©šéœ€è¦ä»€ä¹ˆæ¡ä»¶| UI[Gradio ç•Œé¢]
    UI -->|1. æœç´¢| RAG[RAG æ£€ç´¢å¼•æ“]
    RAG -->|æ£€ç´¢æ³•æ¡| KB[(knowledge_base.json<br>æ°‘æ³•å…¸æ•°æ®åº“)]
    KB -->|è¿”å›: ç¬¬1079æ¡...| RAG
    RAG -->|2. æ‹¼æ¥ Prompt| Model[å¾®è°ƒåçš„ Qwen æ¨¡å‹]
    Model -->|3. ç”Ÿæˆå›ç­”| UI
```

## 3. è¯¦ç»†æ“ä½œæ­¥éª¤

### 3.1 ç¯å¢ƒå®‰è£…

æˆ‘ä»¬éœ€è¦å®‰è£… `gradio` (åšç•Œé¢) å’Œ `sentence-transformers` (åšæœç´¢ï¼Œè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦)ã€‚

1.  åœ¨ `ç¬¬22_24å‘¨` ç›®å½•ä¸‹åˆ›å»º `requirements.txt`ï¼š
    ```text
    gradio
    sentence-transformers
    numpy
    torch
    transformers
    peft
    ```
2.  å®‰è£…ä¾èµ–ï¼š
    ```powershell
    pip install -r ç¬¬22_24å‘¨/requirements.txt
    ```

### 3.2 çŸ¥è¯†åº“å‡†å¤‡

æˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªç®€å•çš„æ³•å¾‹æ•°æ®åº“ã€‚ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ª JSON æ–‡ä»¶ã€‚

1.  åœ¨ `ç¬¬22_24å‘¨` ç›®å½•ä¸‹åˆ›å»º `knowledge_base.json`ï¼š
    ```json
    [
        {
            "id": 1,
            "content": "ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶ä¸ƒåå…­æ¡ï¼šå¤«å¦»åŒæ–¹è‡ªæ„¿ç¦»å©šçš„ï¼Œåº”å½“ç­¾è®¢ä¹¦é¢ç¦»å©šåè®®ï¼Œå¹¶äº²è‡ªåˆ°å©šå§»ç™»è®°æœºå…³ç”³è¯·ç¦»å©šç™»è®°ã€‚"
        },
        {
            "id": 2,
            "content": "ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶ä¸ƒåä¹æ¡ï¼šå¤«å¦»ä¸€æ–¹è¦æ±‚ç¦»å©šçš„ï¼Œå¯ä»¥ç”±æœ‰å…³ç»„ç»‡è¿›è¡Œè°ƒè§£æˆ–è€…ç›´æ¥å‘äººæ°‘æ³•é™¢æèµ·ç¦»å©šè¯‰è®¼ã€‚æœ‰ä¸‹åˆ—æƒ…å½¢ä¹‹ä¸€ï¼Œè°ƒè§£æ— æ•ˆçš„ï¼Œåº”å½“å‡†äºˆç¦»å©šï¼š(ä¸€)é‡å©šæˆ–è€…ä¸ä»–äººåŒå±…ï¼›(äºŒ)å®æ–½å®¶åº­æš´åŠ›æˆ–è€…è™å¾…ã€é—å¼ƒå®¶åº­æˆå‘˜ï¼›(ä¸‰)æœ‰èµŒåšã€å¸æ¯’ç­‰æ¶ä¹ å±¡æ•™ä¸æ”¹ï¼›(å››)å› æ„Ÿæƒ…ä¸å’Œåˆ†å±…æ»¡äºŒå¹´ï¼›(äº”)å…¶ä»–å¯¼è‡´å¤«å¦»æ„Ÿæƒ…ç ´è£‚çš„æƒ…å½¢ã€‚"
        },
        {
            "id": 3,
            "content": "ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶å…«åäº”æ¡ï¼šç¦»å©šåï¼Œå­å¥³ç”±ä¸€æ–¹ç›´æ¥æŠšå…»çš„ï¼Œå¦ä¸€æ–¹åº”å½“è´Ÿæ‹…éƒ¨åˆ†æˆ–è€…å…¨éƒ¨æŠšå…»è´¹ã€‚"
        }
    ]
    ```

### 3.3 ç¼–å†™ RAG å¼•æ“ (`rag_engine.py`)

åˆ›å»ºä¸€ä¸ª Python è„šæœ¬ï¼Œä¸“é—¨è´Ÿè´£â€œæœâ€ã€‚

```python
import json
import numpy as np
from sentence_transformers import SentenceTransformer

class LawRAG:
    def __init__(self, json_path):
        print("ğŸ“¥ æ­£åœ¨åŠ è½½çŸ¥è¯†åº“...")
        with open(json_path, 'r', encoding='utf-8') as f:
            self.knowledge = json.load(f)
        
        # æå–æ‰€æœ‰æ–‡æœ¬
        self.corpus = [item['content'] for item in self.knowledge]
        
        print("ğŸ§  æ­£åœ¨åŠ è½½æ£€ç´¢æ¨¡å‹ (moka-ai/m3e-small)...")
        # ä½¿ç”¨ä¸­æ–‡æ•ˆæœå¾ˆå¥½çš„è½»é‡çº§åµŒå…¥æ¨¡å‹
        self.encoder = SentenceTransformer('moka-ai/m3e-small')
        
        print("âš¡ æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")
        self.corpus_embeddings = self.encoder.encode(self.corpus)

    def search(self, query, top_k=2):
        # 1. æŠŠç”¨æˆ·çš„é—®é¢˜å˜æˆå‘é‡
        query_embedding = self.encoder.encode(query)
        
        # 2. è®¡ç®—ç›¸ä¼¼åº¦ (Cosine Similarity)
        # ç®€å•çš„ç‚¹ç§¯è®¡ç®—
        similarities = np.dot(self.corpus_embeddings, query_embedding)
        
        # 3. æ‰¾å‡ºæœ€ç›¸ä¼¼çš„ top_k ä¸ª
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            results.append(self.corpus[idx])
            
        return results
```

### 3.4 ç¼–å†™ Gradio ç•Œé¢ (`app.py`)

è¿™æ˜¯ä¸»ç¨‹åºï¼Œæ•´åˆäº† RAG å’Œæˆ‘ä»¬ä¹‹å‰è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹ã€‚

```python
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from rag_engine import LawRAG
import os

# --- 1. åˆå§‹åŒ–è·¯å¾„ ---
# è¯·ç¡®ä¿è¿™é‡ŒæŒ‡å‘æ‚¨ç¬¬18_19å‘¨è®­ç»ƒå¥½çš„ lora_output ç›®å½•
LORA_PATH = "../ç¬¬18_19å‘¨/lora_output"
BASE_MODEL = "qwen/Qwen1.5-0.5B"

# --- 2. åŠ è½½æ¨¡å‹ ---
print("ğŸš€ æ­£åœ¨åŠ è½½å¾®è°ƒåçš„ Qwen æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map="auto", trust_remote_code=True)

# åŠ è½½ LoRA æƒé‡
if os.path.exists(LORA_PATH):
    print(f"âœ… æŒ‚è½½ LoRA: {LORA_PATH}")
    model = PeftModel.from_pretrained(model, LORA_PATH)
else:
    print("âš ï¸ æœªæ‰¾åˆ° LoRA æƒé‡ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿è¡Œï¼")

# --- 3. åˆå§‹åŒ– RAG ---
rag = LawRAG(os.path.join(os.path.dirname(__file__), "knowledge_base.json"))

# --- 4. å®šä¹‰å¯¹è¯å‡½æ•° ---
def chat_response(message, history):
    # Step A: æ£€ç´¢çŸ¥è¯†
    retrieved_docs = rag.search(message)
    context = "\n".join(retrieved_docs)
    
    # Step B: æ„å»º Prompt
    prompt = f"""Instruction: ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    
å‚è€ƒèµ„æ–™ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š
{message}

Output: """

    # Step C: æ¨¡å‹æ¨ç†
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    pred = model.generate(**inputs, max_new_tokens=256, temperature=0.7)
    response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=True)
    
    # æ¸…ç†æ‰ Prompt éƒ¨åˆ†ï¼Œåªä¿ç•™å›ç­”
    response = response.split("Output:")[-1].strip()
    
    return response

# --- 5. å¯åŠ¨ç•Œé¢ ---
demo = gr.ChatInterface(
    fn=chat_response,
    title="âš–ï¸ AI æ³•å¾‹é¡¾é—® (RAG + LoRA)",
    description="åŸºäº Qwen-0.5B å¾®è°ƒï¼ŒæŒ‚è½½ã€Šæ°‘æ³•å…¸ã€‹çŸ¥è¯†åº“ã€‚",
    examples=["ç¦»å©šéœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Ÿ", "å¯¹æ–¹å‡ºè½¨äº†æ€ä¹ˆåŠï¼Ÿ", "æŠšå…»è´¹æ€ä¹ˆç®—ï¼Ÿ"]
)

if __name__ == "__main__":
    demo.launch()
```

### 3.5 è¿è¡Œä¸æµ‹è¯•

1.  **è¿è¡Œ**ï¼š
    ```powershell
    python ç¬¬22_24å‘¨/app.py
    ```
2.  **è®¿é—®**ï¼š
    ç»ˆç«¯ä¼šæ˜¾ç¤ºä¸€ä¸ªé“¾æ¥ï¼Œé€šå¸¸æ˜¯ `http://127.0.0.1:7860`ã€‚æŒ‰ä½ Ctrl ç‚¹å‡»æ‰“å¼€ã€‚
3.  **æµ‹è¯•**ï¼š
    è¾“å…¥â€œç¦»å©šéœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Ÿâ€ï¼Œè§‚å¯Ÿå®ƒæ˜¯å¦å¼•ç”¨äº†ã€Šæ°‘æ³•å…¸ã€‹ç¬¬ä¸€åƒé›¶ä¸ƒåä¹æ¡çš„å†…å®¹ã€‚

## 4. éªŒè¯ç›®æ ‡

- [ ] **RAG ç”Ÿæ•ˆ**ï¼šå›ç­”ä¸­åŒ…å«äº†å…·ä½“çš„æ³•æ¡å†…å®¹ï¼ˆå› ä¸ºåŸºç¡€æ¨¡å‹å¯èƒ½èƒŒä¸ä¸‹æ¥è¿™ä¹ˆç»†ï¼‰ã€‚
- [ ] **ç•Œé¢å¯ç”¨**ï¼šGradio ç•Œé¢ç¾è§‚ï¼Œäº¤äº’æµç•…ã€‚
- [ ] **æµç¨‹è·‘é€š**ï¼šæ²¡æœ‰æŠ¥é”™ï¼Œæ¨¡å‹æˆåŠŸåŠ è½½ã€‚

</div>
</div>
