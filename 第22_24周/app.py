import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from rag_engine import LawRAG
import os
import torch


# ç¦ç”¨ä»£ç†ä»¥è§£å†³ Gradio å¯åŠ¨æ—¶çš„ 502/Connection Refused é”™è¯¯
# os.environ["no_proxy"] = "localhost,127.0.0.1,::1"

# --- 1. åˆå§‹åŒ–è·¯å¾„ ---
# å‡è®¾ lora_output åœ¨ ç¬¬18_19å‘¨ ç›®å½•ä¸‹
LORA_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../ç¬¬18_19å‘¨/lora_output"))
BASE_MODEL = r"C:\Users\JYJYJ\.cache\huggingface\hub\models--qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39"

# --- 2. åŠ è½½æ¨¡å‹ ---
print(f"ğŸš€ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {BASE_MODEL}...")
try:
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    # å¼ºåˆ¶ä½¿ç”¨ CPU è¿è¡Œï¼Œé¿å…æ˜¾å­˜é—®é¢˜ï¼Œè™½ç„¶æ…¢ä¸€ç‚¹ä½†ç¨³
    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map="cpu", trust_remote_code=True)
    
    # åŠ è½½ LoRA æƒé‡
    if os.path.exists(LORA_PATH):
        print(f"âœ… æŒ‚è½½ LoRA æƒé‡: {LORA_PATH}")
        model = PeftModel.from_pretrained(model, LORA_PATH)
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° LoRA æƒé‡ ({LORA_PATH})ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿è¡Œï¼")
        
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    # ä¸ºäº†æ¼”ç¤º UIï¼Œè¿™é‡Œä¸é€€å‡ºï¼Œä½†åç»­ä¼šæŠ¥é”™

# --- 3. åˆå§‹åŒ– RAG ---
kb_path = os.path.join(os.path.dirname(__file__), "knowledge_base.json")
if os.path.exists(kb_path):
    rag = LawRAG(kb_path)
else:
    print("âš ï¸ æœªæ‰¾åˆ°çŸ¥è¯†åº“æ–‡ä»¶ï¼ŒRAG åŠŸèƒ½å°†å¤±æ•ˆã€‚")
    rag = None

# --- 4. å®šä¹‰å¯¹è¯å‡½æ•° ---
def chat_response(message, history):
    context = ""
    
    # Step A: æ£€ç´¢çŸ¥è¯†
    if rag:
        try:
            retrieved_docs = rag.search(message)
            context = "\n".join(retrieved_docs)
            print(f"ğŸ” RAG æ£€ç´¢ç»“æœ: {retrieved_docs}")
        except Exception as e:
            print(f"âŒ RAG æ£€ç´¢å‡ºé”™: {e}")
    
    # Step B: æ„å»º Prompt
    # æç®€ Prompt æ¨¡æ¿
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ³•å¾‹åŠ©æ‰‹ã€‚è¯·å®Œå…¨ä¾æ®ä¸‹é¢çš„å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚å¦‚æœå‚è€ƒèµ„æ–™é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´â€œæˆ‘ä¸çŸ¥é“â€ï¼Œä¸è¦ç¼–é€ ã€‚

å‚è€ƒèµ„æ–™ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{message}

å›ç­”ï¼š"""

    # Step C: æ¨¡å‹æ¨ç†
    try:
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # ç”Ÿæˆ
        pred = model.generate(
            **inputs, 
            max_new_tokens=256, 
            temperature=0.7,
            do_sample=True
        )
        
        response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=True)
        
        # ç®€å•æ¸…æ´—ï¼šå°è¯•å»æ‰ prompt éƒ¨åˆ†ï¼Œåªä¿ç•™å›ç­”
        # è¿™é‡Œç”¨ä¸€ä¸ªç®€å•çš„ split ç­–ç•¥ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¤„ç†
        if "å›ç­”ï¼š" in response:
            response = response.split("å›ç­”ï¼š")[-1]
        elif "Output:" in response: # å…¼å®¹è®­ç»ƒæ—¶çš„æ ¼å¼
            response = response.split("Output:")[-1]
            
        return response.strip()
        
    except Exception as e:
        return f"æ¨¡å‹æ¨ç†å‡ºé”™: {str(e)}"

# --- 5. å¯åŠ¨ç•Œé¢ ---
demo = gr.ChatInterface(
    fn=chat_response,
    title="âš–ï¸ AI æ³•å¾‹é¡¾é—® (RAG + LoRA)",
    description="åŸºäº Qwen-0.5B å¾®è°ƒï¼ŒæŒ‚è½½ã€Šæ°‘æ³•å…¸ã€‹çŸ¥è¯†åº“ã€‚è¯·è¾“å…¥æ³•å¾‹é—®é¢˜ï¼Œä¾‹å¦‚â€œç¦»å©šéœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Ÿâ€",
    examples=["ç¦»å©šéœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Ÿ", "å¯¹æ–¹å‡ºè½¨äº†æ€ä¹ˆåŠï¼Ÿ", "æŠšå…»è´¹æ€ä¹ˆç®—ï¼Ÿ"]
)

if __name__ == "__main__":
    demo.launch()
