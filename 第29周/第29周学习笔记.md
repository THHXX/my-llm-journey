# 第29周学习笔记：多模态大模型入门（MiniGPT-4 到 Qwen-VL）

## 1. 任务路径回顾
本周的目标是**实现图文问答系统**，原本计划使用开源的 **MiniGPT-4**，但在实施过程中遇到了硬件瓶颈，最终转向了更适合当前环境且对中文支持更好的国产方案 **Qwen-VL（通义千问-VL）**。

### 核心路径：
1.  **尝试本地部署 MiniGPT-4**：
    *   **知识点**：MiniGPT-4 结合了 ViT（视觉编码器）和 Vicuna（LLM），通过一个投影层（Linear Projection Layer）连接。
    *   **阻碍**：运行 13B 模型需 23GB 显存，7B 模型（8-bit 量化）也需约 12GB 显存。
    *   **结论**：本地核显（Intel Iris Xe）无法运行。

2.  **转向云端 API (DashScope)**：
    *   **选择**：Qwen-VL-Plus（阿里通义千问视觉版）。
    *   **优势**：无需本地算力，原生支持中文，API 调用简单。
    *   **实施**：使用 `dashscope` SDK，通过 `MultiModalConversation` 接口发送图片和文本。

3.  **调试与解决问题**：
    *   **网络问题**：`ProxyError` / `ConnectionResetError`。
        *   *原因*：Python `requests` 库自动读取系统代理设置，导致连接国内阿里云服务失败。
        *   *解决*：代码中强制清除 `http_proxy` 环境变量。
    *   **API 参数问题**：`InvalidParameter`。
        *   *原因*：旧版/OpenAI 格式与 DashScope 原生 SDK 格式不兼容；本地路径处理不当。
        *   *解决*：使用 SDK 原生的 `{'image': ...}` 格式，并确保图片路径为绝对路径 (`file://...`)。

## 2. 核心知识点

### 2.1 多模态大模型 (LMM) 架构
MiniGPT-4 的架构非常有代表性，是学习多模态的经典案例：
*   **Vision Encoder (ViT)**：负责“看”图片，将图片转化为向量特征。
*   **LLM (Vicuna/Llama)**：负责“说”话，具备强大的语言理解和生成能力。
*   **Alignment Layer (投影层)**：核心组件。因为 ViT 输出的特征 LLM 看不懂，需要通过这一层将视觉特征“翻译”（投影）到 LLM 的文本特征空间中。
*   **冻结训练 (Frozen)**：为了节省训练成本，通常冻结 ViT 和 LLM 的参数，只训练中间的投影层。

### 2.2 本地 vs 云端
*   **本地部署 (MiniGPT-4)**：
    *   优点：数据隐私好，无需联网，可深度微调。
    *   缺点：硬件门槛极高（大显存 GPU），环境配置复杂。
*   **云端 API (Qwen-VL)**：
    *   优点：零硬件门槛，开箱即用，模型能力通常更强（Plus/Max 版本）。
    *   缺点：依赖网络，按量付费（通常有免费额度），数据需上传。

### 2.3 Python 开发实战技巧
*   **代理管理**：在脚本中控制环境变量 `os.environ.pop('http_proxy')` 是解决内网/代理冲突的常用手段。
*   **路径处理**：Windows 下的文件路径含有反斜杠 `\`，在 Python 字符串中容易被识别为转义字符（如 `\t`）。
    *   *最佳实践*：使用原始字符串 `r'path\to\file'` 或正斜杠 `'path/to/file'`。

## 3. 代码片段 (Qwen-VL 调用模板)
```python
import os
from http import HTTPStatus
from dashscope import MultiModalConversation

# 1. 禁用代理（直连国内服务）
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)

# 2. 准备数据
image_path = r'你的图片绝对路径.jpg' # 使用绝对路径
question = '图片里有什么？'

# 3. 调用 API
resp = MultiModalConversation.call(
    model='qwen-vl-plus',
    messages=[{
        'role': 'user',
        'content': [
            {'image': f'file://{image_path}'},
            {'text': question}
        ]
    }]
)

# 4. 解析结果
if resp.status_code == HTTPStatus.OK:
    content = resp.output.choices[0]['message']['content']
    print(content[0]['text']) # 输出回答
else:
    print(resp)
```

## 4. 总结
本周虽然没有成功运行本地的 MiniGPT-4（受限于硬件），但通过转向 Qwen-VL，不仅完成了“图文问答”的功能目标，还更深入地理解了多模态模型的应用模式和 API 开发中的常见坑（代理、路径、格式）。这符合“迭代思维”——在一条路不通时，快速寻找替代方案以达成最终目标。
